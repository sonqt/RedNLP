# To Read Papers

This is a list of papers to read in preparation for our 2022 and 2023 summer research in NLP. 

## The development of SQuAD (core)
* SQuAD: 100,000+ Questions for Machine Comprehension of Text [Link](https://arxiv.org/pdf/1606.05250.pdf)
* Adversarial Examples for Evaluating Reading Comprehension Systems [Link](https://arxiv.org/pdf/1707.07328.pdf)
* Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability [Link](https://penzant.net/files/acl2017.pdf)
* What Makes Reading Comprehension Questions Easier? [Link](https://arxiv.org/pdf/1808.09384.pdf)
* SQuAD 2.0: Know what you donâ€™t know: Unanswerable Questions for SQuAD [Link](https://arxiv.org/pdf/1806.03822.pdf)
* Challenges in Information-Seeking QA: Unanswerable questions and Paragraph Retrieval [Link](https://arxiv.org/pdf/2010.11915.pdf)
* Coreference Reasoning in Machine Reading Comprehension [Link](https://arxiv.org/pdf/2012.15573.pdf)
* Benchmarking Machine Reading Comprehension: A Psychological Perspective [Link](https://arxiv.org/pdf/2004.01912.pdf)

## Shortcut learning
* Why Machine Reading Comprehension Models Learn Shortcuts? [Link](https://arxiv.org/pdf/2106.01024.pdf)
* Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models [Link](https://arxiv.org/pdf/2103.06922.pdf)
* Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development [Link](https://arxiv.org/pdf/1906.07132.pdf)
* Which Shortcut Solution Do Question Answering Models Prefer to Learn? [Link](https://arxiv.org/pdf/2211.16220.pdf)
* A Survey on Measuring and Mitigating Reasoning Shortcuts in Machine Reading Comprehension [Link](https://arxiv.org/pdf/2209.01824.pdf)

## MRC techniques
* Improving the Robustness of Question Answering Systems to Question Paraphrasing [Link](https://aclanthology.org/P19-1610.pdf)
* Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension [Link](https://arxiv.org/pdf/2002.00293.pdf)







