# To Read Papers

This is a list of papers to read in preparation for our 2022 summer research in NLP. 

## The development of SQuAD (core)
* SQuAD: 100,000+ Questions for Machine Comprehension of Text [Link](https://arxiv.org/pdf/1606.05250.pdf)
* Reading Wikipedia to Answer Open-Domain Questions [Link](https://arxiv.org/pdf/1704.00051.pdf)
* Adversarial Examples for Evaluating Reading Comprehension Systems [Link](https://arxiv.org/pdf/1707.07328.pdf)
* Evaluation Metrics for Machine Reading Comprehension: Prerequisite Skills and Readability [Link](https://penzant.net/files/acl2017.pdf)
* What Makes Reading Comprehension Questions Easier? [Link](https://arxiv.org/pdf/1808.09384.pdf)
* SQuAD 2.0: Know what you don’t know: Unanswerable Questions for SQuAD [Link](https://arxiv.org/pdf/1806.03822.pdf)
* Challenges in Information-Seeking QA: Unanswerable questions and Paragraph Retrieval [Link](https://arxiv.org/pdf/2010.11915.pdf)
* Different Aspects of Unanswerable questions (based on results in Son’s current researches) 
* Coreference Reasoning in Machine Reading Comprehension [Link](https://arxiv.org/pdf/2012.15573.pdf)
* Benchmarking Machine Reading Comprehension: A Psychological Perspective [Link](https://arxiv.org/pdf/2004.01912.pdf)


## Dataset
* Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension [Link](https://arxiv.org/pdf/2002.00293.pdf)

## Adversarial attacks
* Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models [Link](https://arxiv.org/pdf/2105.11136.pdf)

## Shortcut learning
* Why Machine Reading Comprehension Models Learn Shortcuts? [Link](https://arxiv.org/pdf/2106.01024.pdf)
* Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models [Link](https://arxiv.org/pdf/2103.06922.pdf)
* Avoiding Reasoning Shortcuts: Adversarial Evaluation, Training, and Model Development [Link](https://arxiv.org/pdf/1906.07132.pdf)

## MRC techniques







