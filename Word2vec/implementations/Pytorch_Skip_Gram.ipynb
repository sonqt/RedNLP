{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCP_VUUTBrUU"
      },
      "source": [
        "# **1. Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GExMcWDkC3dh",
        "outputId": "abd86770-be32-4de7-b42d-cbc2253ac5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBU9XpwmCo9F",
        "outputId": "613fdb08-5e88-4c09-b3ef-aee4834f99ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " anarchism originated as a term of abuse first used against early working class radicals including t\n"
          ]
        }
      ],
      "source": [
        "# read in the extracted text file      \n",
        "with open(\"/content/drive/MyDrive/word-embedding-creation/input/text8\", \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "# print out the first 100 characters\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1CXh7PbBpeN"
      },
      "source": [
        "# **2. Preprocess Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qDJAp_dCgDj"
      },
      "source": [
        "Because the corpus is very **large**, if we take all vocabulary the appear in the corpus into consideration, our vocab dictionary grows up to 250,000 words. This cause our models to be unreasonably large and take a large amount of time to train. On the other hand, there are many words that appear only several times in the corpus (< 10); thus, we will not be able to obtain good dense representations for it. Therefore, it is essential for use to only choose our corpus to be the 30000 most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2re8RV4BpGz",
        "outputId": "397cffe0-9982-4bc4-d972-db3d18208a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two', 'is', 'as', 'eight', 'for', 's', 'five', 'three', 'was', 'by', 'that', 'four', 'six', 'seven', 'with', 'on', 'are', 'it', 'from', 'or', 'his', 'an', 'be', 'this', 'which', 'at', 'he', 'also', 'not', 'have', 'were', 'has', 'but', 'other', 'their', 'its', 'first', 'they', 'some', 'had', 'all', 'more', 'most', 'can', 'been', 'such', 'many', 'who', 'new', 'used', 'there', 'after', 'when', 'into', 'american', 'time', 'these', 'only', 'see', 'may', 'than', 'world', 'i', 'b', 'would', 'd', 'no', 'however', 'between', 'about', 'over', 'years', 'states', 'people', 'war', 'during', 'united', 'known', 'if', 'called', 'use', 'th', 'system', 'often', 'state', 'so', 'history', 'will', 'up', 'while', 'where']\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "corpus = text.split()\n",
        "c = Counter(corpus)\n",
        "most_30000 = c.most_common(30000)\n",
        "vocab_dictionary = [i[0] for i in most_30000]\n",
        "print(vocab_dictionary[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvqRdTzxBVAc"
      },
      "outputs": [],
      "source": [
        "int_to_vocab = {ii + 1: word for ii, word in enumerate(vocab_dictionary)}\n",
        "int_to_vocab[0] = '<OOV>'\n",
        "vocab_to_int = {word: ii + 1 for ii, word in enumerate(vocab_dictionary)}\n",
        "vocab_to_int['<OOV>'] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQDbQdHbCo9H",
        "outputId": "9c909a5b-cea9-47e4-b0e8-f2a26d520bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5234, 3081, 12, 6, 195, 2, 3134, 46, 59, 156, 128, 742, 477, 10572, 134, 1, 27350, 2, 1, 103, 855, 3, 1, 15068, 0, 2, 1, 151, 855, 3581]\n",
            "17005207\n"
          ]
        }
      ],
      "source": [
        "int_sequeces = []\n",
        "for word in corpus:\n",
        "    if word in vocab_to_int:\n",
        "        int_sequeces.append(vocab_to_int[word])\n",
        "    else:\n",
        "        int_sequeces.append(0)\n",
        "# int_words = [vocab_to_int[word] for word in words]\n",
        "\n",
        "print(int_sequeces[:30])\n",
        "print(len(int_sequeces))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0psxfJzQCo9I"
      },
      "source": [
        "## Subsampling\n",
        "\n",
        "[(Mikolov et al., 2013)](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)\n",
        "In very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g., “in”, “the”, and “a”). Such words usually provide less information value than the rare words. For example, while the Skip-gram model benefits from observing the co-occurrences of “France” and “Paris”, it benefits much less from observing the frequent co-occurrences of “France” and “the”, as\n",
        "nearly every word co-occurs frequently within a sentence with “the”. This idea can also be applied in the opposite direction; the vector representations of frequent words do not change significantly after training on several million examples.\n",
        "To counter the imbalance between the rare and frequent words, we used a simple subsampling approach: each word $w_i$ in the training set is discarded with probability computed by the formula\n",
        "\n",
        "$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n",
        "\n",
        "where $f(w_i)$ is the frequency of word $w_i$ and $t$ is a chosen threshold, typically around $10^{−5}$. We chose this subsampling formula because it aggressively subsamples words whose frequency is greater than t while preserving the ranking of the frequencies. Although this subsampling formula was chosen heuristically, we found it to work well in practice. It accelerates learning and even significantly improves the accuracy of the learned vectors of the rare words, as will be shown in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5hfpRo7Co9I",
        "outputId": "b2a71ebd-182a-4bc3-c8e9-e784507f5bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[742, 10572, 27350, 15068, 855, 3581, 195, 5, 10713, 1325, 105, 59, 2732, 363, 3673, 7089, 28, 2878, 603, 1135, 8984, 32, 4148, 6438, 5234, 345, 4861, 6754, 7574, 11065]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "threshold = 1/10**5                              # t in the formula\n",
        "\n",
        "word_counts = Counter(int_sequeces)             # for calculating $f(w_i)$\n",
        "total_count = len(int_sequeces)                 \n",
        "freqs = {word: count/total_count for word, count in word_counts.items()}        # dictionary of every word's frequency\n",
        "\n",
        "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}     # dictionary of every word's probability of discarding\n",
        "train_words = [word for word in int_sequeces if random.random() < (1 - p_drop[word])]\n",
        "\n",
        "print(train_words[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5Uc70GCCo9I"
      },
      "source": [
        "## 3. Making batches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to Mikolov et al., 2013\n",
        "\n",
        "\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose $C=5$, for each training word we will select randomly a number $R$ in range $[1 : C]$, and then use $R$ words from history and  words from the future of the current word as correct labels.\"\n"
      ],
      "metadata": {
        "id": "XIAR9vdj-Yh2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Av2fZFLGmn-"
      },
      "outputs": [],
      "source": [
        "def get_labels(words, idx, window_size=5):\n",
        "    ''' Get a list of context words (which is also target words in training phase) in a window around a center word. '''\n",
        "    \n",
        "    R = np.random.randint(1, window_size+1)\n",
        "    start = idx - R if (idx - R) > 0 else 0\n",
        "    stop = idx + R\n",
        "    labels = [word for word in words[start:idx] + words[idx+1:stop+1] if word != 0]\n",
        "    \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tcexHpRRCo9J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "def create_iterative_batches(words, batch_size, window_size=3):\n",
        "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
        "    \n",
        "    n_batches = len(words)//batch_size\n",
        "    \n",
        "    # we discard some last tokens that are not in any batches\n",
        "    words = words[:n_batches*batch_size]\n",
        "    \n",
        "    for idx in range(0, len(words), batch_size):\n",
        "        x, y = [], []\n",
        "        batch = words[idx:idx+batch_size]\n",
        "        for ii in range(len(batch)):\n",
        "            batch_x = batch[ii]\n",
        "            batch_y = get_labels(batch, ii, window_size)\n",
        "            y.extend(batch_y)\n",
        "            x.extend([batch_x]*len(batch_y))\n",
        "        yield x, y\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7BobW79Co9K"
      },
      "source": [
        "---\n",
        "## Validation\n",
        "\n",
        "Here we use cosine similarity to track current performances of training models.\n",
        "\n",
        "$$\n",
        "\\mathrm{similarity} = \\cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XIlKlFOgCo9K"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n",
        "    \"\"\" Returns the cosine similarity of validation words with words in the embedding matrix.\n",
        "        Here, embedding should be a PyTorch embedding module.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Here we're calculating the cosine similarity between some random words and \n",
        "    # our embedding vectors. With the similarities, we can look at what words are\n",
        "    # close to our random words.\n",
        "    \n",
        "    # sim = (a . b) / |a||b|\n",
        "    \n",
        "    embed_vectors = embedding.weight\n",
        "    \n",
        "    # magnitude of embedding vectors, |b|\n",
        "    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n",
        "    \n",
        "    # pick N words from our ranges (0,window) and (1000,1000+window). lower id implies more frequent \n",
        "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
        "    valid_examples = np.append(valid_examples,\n",
        "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
        "    valid_examples = torch.LongTensor(valid_examples).to(device)\n",
        "    \n",
        "    valid_vectors = embedding(valid_examples)\n",
        "    similarities = torch.mm(valid_vectors, embed_vectors.t())/magnitudes\n",
        "        \n",
        "    return valid_examples, similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3JJEIKvCo9K"
      },
      "source": [
        "## SkipGram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ryKAaM-CCo9L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, n_vocab, n_embed):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed = nn.Embedding(n_vocab, n_embed)\n",
        "        self.output = nn.Linear(n_embed, n_vocab)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.embed(x)\n",
        "        scores = self.output(x)\n",
        "        log_ps = self.log_softmax(scores)\n",
        "        \n",
        "        return log_ps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmrYzFGaCo9L",
        "scrolled": false,
        "outputId": "a39070f9-a54e-4915-948b-d3f97c585e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2500 9.861569799423219\n",
            "called | stables, characteristic, zeus, violated, hexagonal\n",
            "people | turing, indo, baptized, relieving, religions\n",
            "has | comprehend, moors, tripolitania, pygmy, colonial\n",
            "this | resource, tutorials, lifeforms, regulations, deism\n",
            "with | auchinleck, spectator, far, ouest, retain\n",
            "while | requirements, tao, livery, conspirators, macintoshes\n",
            "states | aroma, hardcover, mehmet, shetland, trappings\n",
            "war | diocletian, alexandra, babur, zedong, unintended\n",
            "bible | autopsy, morrison, canon, testament, eclectic\n",
            "recorded | voters, misunderstood, millet, winter, guaranteeing\n",
            "defense | fief, season, pants, aeolus, monarchy\n",
            "frac | decoding, solution, deuce, claim, vendors\n",
            "professional | chan, heinrich, pyle, ringing, musketeers\n",
            "freedom | underway, mature, restructured, determining, animist\n",
            "smith | matteo, mansfield, rodrigo, summaries, stamp\n",
            "additional | monteverdi, frontal, retained, solstice, arbor\n",
            "...\n",
            "5000 9.555713206100464\n",
            "and | finally, gems, stadiums, zbigniew, goshen\n",
            "is | correspond, elapsed, runic, deductive, disregarded\n",
            "called | stables, hexagonal, zeus, characteristic, lag\n",
            "first | ralph, weekend, fairfax, corner, oval\n",
            "if | contact, formations, polemical, waist, case\n",
            "have | doorway, trust, groups, dancing, summoning\n",
            "in | particularly, cooley, weeks, romano, prospered\n",
            "who | realizes, grandson, simile, vaughan, decided\n",
            "bible | scripture, testament, hebrew, autopsy, canon\n",
            "scale | icy, parcels, formulating, embouchure, mass\n",
            "pressure | envisaged, plutonium, lanthanum, vapor, taxable\n",
            "magazine | faq, str, etienne, fu, outs\n",
            "ocean | kilometers, km, adjoining, mountainous, coordinates\n",
            "stage | caesar, roda, blending, differentiates, replay\n",
            "powers | unanimously, gauls, bicameral, uneasy, graveyard\n",
            "universe | ability, dystopian, paradigms, convoluted, synodic\n",
            "...\n",
            "7500 9.342948834609984\n",
            "no | compile, trust, iridium, croquet, nothing\n",
            "six | four, eight, two, one, three\n",
            "this | prayed, instituting, anselm, leap, netherlands\n",
            "after | baudelaire, march, nassau, defeat, ghent\n",
            "however | appealing, individualistic, implement, prompted, rescinded\n",
            "there | austronesian, entirely, equates, tripartite, literacy\n",
            "the | of, ezra, held, reelection, clergy\n",
            "seven | one, two, five, eight, zero\n",
            "account | inherit, analyses, period, consciousness, complicity\n",
            "dr | mckellen, martini, tenor, josef, gropius\n",
            "taking | vain, compelled, vikings, circumcision, froze\n",
            "accepted | arukh, controversy, dispensation, bukhari, gardener\n",
            "frac | riemann, cdot, x, rings, deuce\n",
            "instance | simplifying, bayonet, unitary, manipulation, expressing\n",
            "engineering | supplier, advances, abet, decoding, wt\n",
            "mainly | communities, subsistence, ellipse, descriptive, mauritanian\n",
            "...\n",
            "10000 9.251052937316894\n",
            "who | realizes, chose, vaughan, grandson, righteousness\n",
            "such | use, irn, amides, synthesizing, some\n",
            "however | implement, correlated, appealing, individualistic, not\n",
            "many | variety, more, are, restaurants, tastes\n",
            "the | partial, judea, subatomic, flows, place\n",
            "its | geodetic, dma, mainframes, hyderabad, much\n",
            "will | to, cannot, go, simply, want\n",
            "can | be, agonists, if, homeomorphic, detect\n",
            "event | admittedly, pileser, coronary, torah, eq\n",
            "alternative | traversal, comprehension, hints, vitro, probabilistic\n",
            "pre | promotions, farley, supply, inquirer, tacitus\n",
            "san | manuel, california, mancini, chargers, antonio\n",
            "placed | drilling, pitch, mail, labours, pyramids\n",
            "applied | orientations, logic, ranging, form, fundamental\n",
            "additional | shortcuts, another, relaxed, kl, amplifiers\n",
            "nobel | prize, chemist, laureate, geneticist, dalton\n",
            "...\n",
            "12500 9.178843148231506\n",
            "on | march, bbc, ankle, september, hearst\n",
            "his | he, him, her, son, had\n",
            "are | or, many, all, different, there\n",
            "to | able, will, they, requires, wants\n",
            "known | also, histone, mayan, banu, called\n",
            "used | use, commonly, similar, often, numeral\n",
            "at | hartford, courses, norwich, dumped, residences\n",
            "however | implement, appealing, correlated, abolish, individualistic\n",
            "ocean | atlantic, seismic, reefs, coast, islands\n",
            "existence | lyceum, sense, notion, realism, means\n",
            "universe | cosmologists, dystopian, aether, cosmological, our\n",
            "resources | cyber, knowledge, unavailable, extract, agricultural\n",
            "file | zip, implementation, browsers, headers, data\n",
            "operating | ported, unix, versions, solaris, ms\n",
            "mean | geometric, arithmetic, somewhere, lebesgue, generalize\n",
            "hold | sacrificial, certain, trinitarian, priesthood, alters\n",
            "...\n",
            "15000 9.127641983795167\n",
            "more | than, many, less, scale, often\n",
            "american | actor, singer, musician, playwright, songwriter\n",
            "he | his, him, she, friend, loved\n",
            "some | are, many, most, were, small\n",
            "with | joining, afl, defending, auchinleck, has\n",
            "of | the, in, and, main, presently\n",
            "d | b, <OOV>, playwright, composer, physicist\n",
            "these | have, are, hypothetical, separate, most\n",
            "institute | university, postgraduate, logan, school, research\n",
            "hit | pop, hits, singles, billboard, hitting\n",
            "joseph | anthony, etienne, hiram, personally, sidney\n",
            "pope | xiii, papacy, papal, vii, xii\n",
            "creation | finkelstein, mexica, origins, creationism, mysticism\n",
            "dr | physician, lothar, covert, willi, chronic\n",
            "discovered | jpl, redshift, discoveries, uranium, dwarf\n",
            "notes | sounding, fet, digraph, miscellaneous, narrowed\n",
            "...\n",
            "17500 9.092725958251954\n",
            "have | are, these, there, but, been\n",
            "called | the, is, interact, known, nanotubes\n",
            "nine | one, eight, four, seven, two\n",
            "than | less, more, louder, larger, longer\n",
            "united | states, kingdom, grants, uruguay, nations\n",
            "there | are, have, although, all, always\n",
            "when | before, immediately, so, quickly, only\n",
            "american | actor, singer, musician, canadian, songwriter\n",
            "brother | father, son, daughter, succeeded, throne\n",
            "frac | theta, cdot, equation, x, gamma\n",
            "quite | extremely, necks, fifths, confused, parallel\n",
            "troops | army, forces, war, armies, retreat\n",
            "applications | software, systems, unix, user, messages\n",
            "resources | agricultural, cyber, knowledge, environmental, academies\n",
            "bill | brooke, johnson, patricia, vaughan, cindy\n",
            "san | diego, california, francisco, antonio, mancini\n",
            "...\n",
            "20000 9.050374101257324\n",
            "its | geodetic, gains, perestroika, has, government\n",
            "were | their, still, been, these, have\n",
            "new | york, scribner, founded, franchise, dover\n",
            "called | is, form, known, which, sometimes\n",
            "as | such, sometimes, considered, which, also\n",
            "will | you, want, must, so, do\n",
            "so | it, if, we, be, would\n",
            "up | barely, off, teg, lined, half\n",
            "engine | engines, fuel, combustion, engined, diesel\n",
            "behind | caught, tough, opponent, ground, swept\n",
            "operations | operation, implementing, subtraction, module, applets\n",
            "road | tha, paddington, bridge, thames, highway\n",
            "rise | revival, dominates, turmoil, socialism, pentecostal\n",
            "mean | geometric, root, above, arithmetic, approximated\n",
            "pressure | vapor, atmospheric, concentrations, heat, flow\n",
            "shows | show, modulating, sitcom, sitcoms, aired\n",
            "...\n",
            "22500 9.029671611213685\n",
            "th | century, nd, rd, early, beginning\n",
            "history | historical, origins, see, overview, references\n",
            "see | links, external, disambiguation, topics, references\n",
            "six | eight, four, two, three, one\n",
            "new | york, scribner, founded, franchise, dover\n",
            "up | thin, would, lined, ground, penetrate\n",
            "four | three, two, one, seven, six\n",
            "such | often, examples, as, are, include\n",
            "square | corner, sq, kilometres, occupies, acres\n",
            "additional | required, such, include, stored, store\n",
            "joseph | hiram, jr, john, anthony, woodward\n",
            "alternative | pseudoscience, bang, creationism, discussion, traditional\n",
            "troops | army, forces, soldiers, war, allied\n",
            "ice | hydrophobic, rink, sticks, layers, glaciers\n",
            "paris | france, de, la, en, manet\n",
            "articles | com, pages, information, site, page\n",
            "...\n",
            "25000 9.004180244827271\n",
            "an | a, instance, of, full, every\n",
            "s | nine, four, zero, eight, one\n",
            "history | external, references, see, historical, origins\n",
            "when | before, immediately, so, agony, waiting\n",
            "six | four, eight, five, one, seven\n",
            "i | ve, we, myself, t, let\n",
            "his | he, him, himself, her, friend\n",
            "which | the, comprises, only, of, separate\n",
            "taking | pains, lead, place, embarrassed, embark\n",
            "institute | university, school, technology, studies, logan\n",
            "http | www, htm, com, edu, org\n",
            "magazine | magazines, monthly, published, weekly, wired\n",
            "accepted | scholars, rejected, arukh, syrians, bukhari\n",
            "orthodox | orthodoxy, catholic, patriarch, jewish, christian\n",
            "primarily | including, mostly, variety, and, mainly\n",
            "creation | creationism, understandings, cosmology, mysticism, malevolent\n",
            "...\n",
            "27500 8.989196953582764\n",
            "such | often, are, other, useful, as\n",
            "state | states, law, federal, alabama, delta\n",
            "had | was, soon, abandoned, were, did\n",
            "been | has, recent, had, some, have\n",
            "two | three, five, zero, four, six\n",
            "most | widely, many, among, prevalent, less\n",
            "one | seven, nine, eight, four, five\n",
            "all | are, there, every, any, number\n",
            "smith | brenda, keith, david, singer, weaver\n",
            "alternative | creationism, pseudoscience, faq, bang, proponents\n",
            "governor | appointed, governors, commander, president, lieutenant\n",
            "fall | summer, during, unrest, catastrophe, moderate\n",
            "instance | can, examples, example, similar, is\n",
            "accepted | validity, rejected, resolved, accept, hadith\n",
            "primarily | mostly, including, especially, variety, mainly\n",
            "bible | testament, septuagint, scripture, prophets, biblical\n",
            "...\n",
            "30000 8.965351349830627\n",
            "also | see, most, in, uses, include\n",
            "often | especially, sometimes, some, particularly, used\n",
            "<OOV> | known, seven, eight, one, po\n",
            "were | thousands, they, had, consisted, been\n",
            "while | generally, themselves, increasingly, tend, alike\n",
            "i | ve, we, say, myself, you\n",
            "all | there, every, numbers, non, choose\n",
            "after | before, during, ended, lasted, shortly\n",
            "active | passive, present, headquartered, targeting, several\n",
            "derived | derives, etymology, called, derive, word\n",
            "paris | france, la, nantes, en, de\n",
            "bill | johnson, senator, clinton, brooke, john\n",
            "mean | geometric, arithmetic, deviation, comparing, variance\n",
            "animals | animal, pets, insects, habits, eat\n",
            "institute | university, school, technology, education, sciences\n",
            "ocean | atlantic, oceanic, coast, islands, winds\n",
            "...\n",
            "32500 8.9527499917984\n",
            "used | use, commonly, often, or, example\n",
            "the | in, of, from, which, a\n",
            "may | or, certain, cases, to, should\n",
            "their | they, themselves, them, allowed, were\n",
            "also | see, other, and, in, an\n",
            "an | is, a, which, instance, also\n",
            "most | many, especially, in, particularly, widely\n",
            "world | nations, europe, germany, nation, debuted\n",
            "existence | argument, claim, teleological, deny, reason\n",
            "stage | improv, completing, blossom, undergo, theatre\n",
            "running | ran, run, unix, tackle, runs\n",
            "award | awards, best, emmy, honors, pulitzer\n",
            "operations | operation, implementing, addition, bitwise, subtraction\n",
            "ice | sticks, periods, polar, glaciers, melted\n",
            "units | unit, density, si, redefined, gallon\n",
            "operating | unix, ported, dos, ms, microsoft\n",
            "...\n",
            "35000 8.937729008293152\n",
            "when | once, before, it, so, quickly\n",
            "united | states, kingdom, nations, uruguay, america\n",
            "time | before, spent, first, began, next\n",
            "zero | two, four, five, three, six\n",
            "at | reaches, near, around, before, first\n",
            "so | if, be, will, it, cannot\n",
            "d | b, laureate, composer, physicist, fran\n",
            "people | living, americans, ethnicity, politicians, buddhists\n",
            "shows | show, sitcoms, aired, sitcom, tv\n",
            "resources | resource, exploited, source, timber, exploitation\n",
            "operations | operation, bitwise, implementing, subtraction, intelligence\n",
            "mainly | primarily, concentrated, predominantly, relatively, mostly\n",
            "road | roads, highway, motorway, paths, freeway\n",
            "something | anything, feel, you, feels, think\n",
            "square | occupies, acres, kilometres, sq, located\n",
            "behind | kicked, ball, tee, ground, caught\n",
            "...\n",
            "37500 8.928569122314453\n",
            "first | nine, was, second, after, one\n",
            "the | of, from, part, in, through\n",
            "more | than, less, much, many, higher\n",
            "up | eventually, out, tear, half, away\n",
            "state | states, federal, united, capital, government\n",
            "which | the, comprises, other, essentially, consists\n",
            "was | had, later, after, until, subsequently\n",
            "years | zero, five, birth, age, male\n",
            "professional | basketball, wrestler, tennis, specialist, hockey\n",
            "dr | friend, chemist, rapper, willi, irwin\n",
            "lived | he, moved, parents, emigrated, grew\n",
            "http | www, htm, com, html, edu\n",
            "resources | resource, source, exploited, extensive, timber\n",
            "stage | actors, dramatic, improv, theatre, musicals\n",
            "pope | papal, xiii, papacy, popes, xii\n",
            "marriage | marriages, divorce, married, marry, divorced\n",
            "...\n",
            "40000 8.923702442169189\n",
            "as | such, considered, unlike, often, sometimes\n",
            "had | was, were, soon, remained, whom\n",
            "its | which, the, has, itself, however\n",
            "the | of, which, in, from, a\n",
            "use | used, using, or, usage, commonly\n",
            "war | troops, allied, fought, surrender, battle\n",
            "than | less, more, compared, fewer, greater\n",
            "between | differences, correspond, disagreements, relationship, separated\n",
            "dr | friend, miller, thornton, hawthorne, harvey\n",
            "operating | unix, ported, dos, ibm, ms\n",
            "numerous | several, major, extensive, including, especially\n",
            "shows | show, aired, sitcom, sitcoms, modulating\n",
            "existence | argument, possibility, teleological, ontological, deny\n",
            "prince | princess, son, crown, king, saxe\n",
            "universe | cosmology, cosmological, universes, bang, multiverse\n",
            "ice | sticks, glaciers, winter, rocks, polar\n",
            "...\n",
            "42500 8.909896557426453\n",
            "zero | two, four, five, three, six\n",
            "years | months, year, until, after, eighteen\n",
            "in | of, the, from, western, and\n",
            "b | d, writer, composer, one, politician\n",
            "some | other, often, have, many, such\n",
            "its | which, within, has, region, eastern\n",
            "to | able, but, would, that, they\n",
            "often | sometimes, especially, occasionally, some, particularly\n",
            "pressure | pressures, cooling, tube, mpa, vanes\n",
            "consists | consisting, consist, comprises, include, composed\n",
            "bbc | news, april, day, headline, cnn\n",
            "powers | sovereign, dominion, sovereignty, exercised, possess\n",
            "placed | placing, vertical, angled, placement, vertically\n",
            "police | officers, personnel, criminals, guards, arrested\n",
            "question | questions, answer, doubts, validity, whether\n",
            "bill | senator, clinton, johnson, george, actor\n",
            "...\n",
            "45000 8.900204228401185\n",
            "six | eight, four, seven, one, five\n",
            "time | until, continued, started, began, year\n",
            "see | links, external, article, list, references\n",
            "s | nine, eight, seven, one, four\n",
            "world | europe, nations, history, opec, qualifier\n",
            "their | themselves, they, those, own, retain\n",
            "two | three, zero, four, five, six\n",
            "<OOV> | nica, po, la, brazilian, pl\n",
            "marriage | marriages, married, divorce, divorced, marry\n",
            "road | roads, highway, motorway, traffic, routes\n",
            "mainly | predominantly, primarily, concentrated, mostly, western\n",
            "magazine | magazines, newspaper, monthly, weekly, interview\n",
            "engine | engines, combustion, diesel, powered, fuel\n",
            "grand | petit, muscovy, juries, knights, ducal\n",
            "lived | fled, emigrated, spent, survived, ancestors\n",
            "something | you, anything, what, isn, doesn\n",
            "...\n",
            "47500 8.889103465461732\n",
            "no | yes, duplicate, insubstantial, does, gon\n",
            "some | are, other, many, such, often\n",
            "eight | six, one, seven, four, five\n",
            "be | can, cannot, that, not, so\n",
            "he | his, him, himself, friend, she\n",
            "there | are, all, have, least, exists\n",
            "united | states, kingdom, australia, america, uruguay\n",
            "states | united, u, state, kingdom, establishments\n",
            "engineering | engineers, biomedical, research, interdisciplinary, technical\n",
            "brother | son, father, nephew, cousin, uncle\n",
            "centre | courtyard, tourist, inverness, town, glasgow\n",
            "account | accounts, sources, according, book, mentioned\n",
            "http | htm, www, com, org, html\n",
            "woman | girl, children, mother, female, she\n",
            "prince | princess, son, elizabeth, isabella, king\n",
            "derived | derives, derive, etymology, meaning, called\n",
            "...\n",
            "50000 8.892140551376343\n",
            "into | newly, loose, eventually, enter, meanwhile\n",
            "used | commonly, use, using, uses, or\n",
            "some | many, have, these, other, often\n",
            "zero | two, five, four, three, seven\n",
            "b | d, writer, mathematician, laureate, composer\n",
            "by | and, initially, paul, pierre, was\n",
            "see | external, references, links, list, article\n",
            "is | means, above, takes, called, depending\n",
            "institute | university, technology, laboratory, research, sciences\n",
            "bbc | news, channel, times, npr, headline\n",
            "paris | france, seine, marseille, la, brussels\n",
            "proposed | proposals, principle, proposal, formulating, proposes\n",
            "cost | costs, incentive, price, usd, incurred\n",
            "governor | appointed, governors, commander, president, lieutenant\n",
            "grand | petit, duchy, ducal, luxembourg, muscovy\n",
            "question | questions, answer, whether, doubts, validity\n",
            "...\n",
            "52500 8.883240573883057\n",
            "one | four, seven, eight, nine, three\n",
            "an | a, is, implies, field, which\n",
            "were | these, many, had, they, thousands\n",
            "has | been, since, recent, however, recently\n",
            "states | united, state, kingdom, countries, nations\n",
            "than | less, more, compared, fewer, faster\n",
            "b | d, mathematician, laureate, politician, writer\n",
            "eight | seven, six, one, four, five\n",
            "magazine | magazines, newspaper, weekly, monthly, interview\n",
            "derived | derives, derive, etymology, meaning, word\n",
            "mean | defined, does, deviation, is, arithmetic\n",
            "professional | basketball, professionals, teams, tennis, amateur\n",
            "marriage | marriages, divorce, married, divorced, husband\n",
            "know | you, think, we, really, do\n",
            "assembly | legislative, seats, bicameral, elections, parliament\n",
            "account | accounts, sources, according, explanations, credit\n",
            "...\n",
            "55000 8.878588192367554\n",
            "there | have, are, all, many, but\n",
            "war | troops, forces, army, battle, invasion\n",
            "had | was, subsequently, were, finally, came\n",
            "known | referred, originally, called, <OOV>, also\n",
            "two | four, three, five, six, seven\n",
            "not | that, should, but, cannot, be\n",
            "th | nd, rd, century, st, early\n",
            "eight | one, six, seven, four, five\n",
            "http | htm, www, com, html, org\n",
            "test | tests, testing, cricket, tested, match\n",
            "quite | even, somewhat, fairly, more, seem\n",
            "numerous | including, several, many, especially, various\n",
            "experience | subjective, mental, experiences, sensations, impressions\n",
            "creation | genesis, humankind, mankind, creationism, olam\n",
            "older | families, younger, age, median, household\n",
            "running | ran, run, operating, tackle, unix\n",
            "...\n",
            "57500 8.87687597026825\n",
            "states | united, state, kingdom, u, federated\n",
            "b | d, laureate, e, mathematician, one\n",
            "two | three, zero, four, five, six\n",
            "system | systems, based, operating, pc, portable\n",
            "eight | one, six, seven, five, four\n",
            "will | must, your, if, cannot, should\n",
            "his | he, himself, him, friend, brother\n",
            "they | their, them, do, themselves, could\n",
            "governor | appointed, governors, commander, minister, lieutenant\n",
            "derived | derive, derives, etymology, origin, meaning\n",
            "placed | placement, placing, vertically, which, shape\n",
            "powers | sovereign, exercised, government, sovereignty, governments\n",
            "troops | army, forces, surrendered, armies, soldiers\n",
            "running | ran, run, operating, platforms, hack\n",
            "existence | deny, argument, metaphysical, possibility, belief\n",
            "ice | glacier, rocks, glaciers, glacial, rock\n",
            "...\n",
            "60000 8.862408811569214\n",
            "this | thus, be, that, actual, it\n",
            "first | after, before, later, second, finally\n",
            "called | form, is, a, forming, derived\n",
            "can | if, be, example, or, cannot\n",
            "in | also, and, the, from, most\n",
            "for | using, such, addition, include, application\n",
            "such | other, often, many, include, are\n",
            "than | less, more, compared, even, faster\n",
            "taking | prescribed, mart, rope, waiting, advantage\n",
            "shown | is, shows, where, show, can\n",
            "ocean | atlantic, oceanic, waters, coast, island\n",
            "applications | systems, useful, computers, functionality, software\n",
            "consists | consisting, comprises, consist, divided, subdivided\n",
            "joseph | smith, john, jr, william, paul\n",
            "grand | muscovy, ducal, duchy, raced, petit\n",
            "active | maintains, passive, presence, headquartered, currently\n",
            "...\n",
            "62500 8.870790477371216\n",
            "was | after, soon, s, had, until\n",
            "and | of, including, mainly, in, with\n",
            "is | a, it, above, called, example\n",
            "two | three, four, zero, five, six\n",
            "after | soon, during, last, took, shortly\n",
            "this | that, thus, be, it, consider\n",
            "war | troops, army, outbreak, invasion, conflict\n",
            "than | less, compared, more, slightly, even\n",
            "prince | princess, king, eldest, son, crown\n",
            "road | roads, highway, motorway, streets, street\n",
            "accepted | canonization, accept, universally, submitted, rejected\n",
            "nobel | prize, laureate, biologist, chemist, recipient\n",
            "fall | fell, during, lasts, slowed, fallen\n",
            "award | awards, best, earned, oscar, emmy\n",
            "bbc | news, channel, day, programmes, cnn\n",
            "assembly | parliament, seats, legislative, elections, bicameral\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "# check if GPU is available\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "embedding_dim=100 \n",
        "\n",
        "model = SkipGram(len(vocab_to_int), embedding_dim).to(device)\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "# With pytorch, we need to manually print the training stats to keep track of the progress\n",
        "print_every = 2500\n",
        "steps = 0\n",
        "epochs = 15\n",
        "\n",
        "running_loss = 0.0\n",
        "for e in range(epochs):\n",
        "    # get input and target batches\n",
        "    for inputs, targets in create_iterative_batches(train_words, 1024):\n",
        "        steps += 1\n",
        "        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        log_ps = model(inputs)\n",
        "        loss = criterion(log_ps, targets)\n",
        "        running_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if steps % print_every == 0:                  \n",
        "            # getting examples and similarities      \n",
        "            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n",
        "            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n",
        "            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n",
        "            \n",
        "            print(steps, running_loss/2500)\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for ii, valid_idx in enumerate(valid_examples):\n",
        "                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n",
        "                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n",
        "            print(\"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXez-QMbEJ1B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "numpy_embedding = model.embed.weight.detach().to('cpu').numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPmGaDVoK7UJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b59ebd-149d-456d-bf22-e3c825437e7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30001, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "numpy_embedding.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYot9kGi_P2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4395da31-15fb-40ea-fc2e-96a567da9203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 2808\n",
            "Total: 13537\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "def find_cosine_similarity(vector_matrix, v):\n",
        "    \"\"\"\n",
        "    vector_matrix: pytorch tensor (matrix) (word representations for the whole vocabulary dictionary)\n",
        "    v: pytorch tensor (vector we need to find best consine similarity score for)\n",
        "    \"\"\"\n",
        "    # find the dot product between u and v \n",
        "    dot = torch.matmul(vector_matrix, torch.transpose(v.unsqueeze(0),0,1))\n",
        "    # find the L2 norm of u \n",
        "    norm_u = torch.sqrt(torch.sum(vector_matrix**2, dim=1))\n",
        "    # Compute the L2 norm of v\n",
        "    norm_v = torch.sqrt(torch.sum((v**2).unsqueeze(0), dim=1))\n",
        "    # Compute the cosine similarity\n",
        "    norm_u = norm_u.unsqueeze(1)\n",
        "    cosine_sim = dot/norm_u/norm_v\n",
        "    \n",
        "    return cosine_sim\n",
        "\n",
        "def find_analogy(word_a, word_b, word_c, words_vocabulary, vector_matrix):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "\n",
        "        word_a, word_b, word_c: Python string (The three input words of word analogy test)\n",
        "        words_vocabulary: Python dictionary (The dictionary mapping vocaburay into index)\n",
        "        vector_matrix: Pytorch tensor(matrix) (word representations for the whole vocabulary dictionary)\n",
        "    Return:\n",
        "    \n",
        "        best_word: The best word predicted given pretrained representations\n",
        "    \"\"\"\n",
        "\n",
        "    # convert words to lower case\n",
        "    word_a = word_a.lower()\n",
        "    word_b = word_b.lower()\n",
        "    word_c = word_c.lower()\n",
        "    \n",
        "    # Find the word embeddings for word_a, word_b, word_c\n",
        "    word_a = words_vocabulary[word_a]\n",
        "    word_b = words_vocabulary[word_b]\n",
        "    word_c = words_vocabulary[word_c]\n",
        "    e_a, e_b, e_c = vector_matrix[word_a], vector_matrix[word_b], vector_matrix[word_c]\n",
        "\n",
        "    # Compute cosine similarity between the vectors u and v\n",
        "    # u:(e_b - e_a) \n",
        "    # v:((w's vector representation) - e_c)\n",
        "\n",
        "    # Calculate all similarity score over the whole dictionary\n",
        "    cosine_sim = find_cosine_similarity(vector_matrix, e_b - e_a + e_c)\n",
        "    \n",
        "    # eliminate word_a, word_b, word_c from options for prediction\n",
        "    cosine_sim[word_a] = -float(\"Inf\")\n",
        "    cosine_sim[word_b] = -float(\"Inf\")\n",
        "    cosine_sim[word_c] = -float(\"Inf\")\n",
        "    cosine_sim[0] = -float(\"Inf\")\n",
        "\n",
        "    # use argmax to find word with highest similarity score\n",
        "    max_index = torch.argmax(cosine_sim)\n",
        "    max_index = int(max_index)\n",
        "    for word in words_vocabulary:\n",
        "        if words_vocabulary[word] == max_index:\n",
        "            best_word = word\n",
        "            break\n",
        "    return best_word\n",
        "\n",
        "# for taking input from the user and doing word analogy task on that\n",
        "def word_analogy_test():\n",
        "    # Load pretrained word representations from txt file\n",
        "    words_vocabulary, vector_matrix = vocab_to_int, torch.from_numpy(numpy_embedding)\n",
        "    inp = []\n",
        "    out = []\n",
        "\n",
        "    # Get testing samples from txt file\n",
        "    with open(\"drive/MyDrive/NLP-2021/Project1-NLP/WordAnalogy-GLoVe/data/data.txt\") as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            if line[0].isalpha():\n",
        "                line = line[:-1]\n",
        "                data = line.split()\n",
        "                for i in range(len(data)):\n",
        "                    data[i] = data[i].lower()\n",
        "                inp.append(data[:3])\n",
        "                out.append(data[3])\n",
        "    f.close()\n",
        "    # Count correct tests\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i in range(len(inp)):\n",
        "      # because we only use 30,000 most frequent words in our datasets, there are cases that words in samples are not in our vocabulary dictionary\n",
        "        try:\n",
        "            best_pick = find_analogy(*inp[i], words_vocabulary, vector_matrix)\n",
        "            if out[i].lower() not in list(words_vocabulary.keys()):\n",
        "                pass\n",
        "            if best_pick == out[i].lower():\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        except:\n",
        "            pass\n",
        "    print(\"Correct:\", correct)\n",
        "    print(\"Total:\", total)\n",
        "word_analogy_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_rrxYqmPCo9L"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Pytorch Skip_Gram.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}