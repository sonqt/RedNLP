{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjXjmdWbJtjK"
      },
      "source": [
        "## **1. The corpus**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we load our folders in our Google Drive into Google Colab."
      ],
      "metadata": {
        "id": "5MAg952z20SB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy540X2eQ9Ai",
        "outputId": "6b7e2e8b-50c6-44ee-a3f5-494fba5f58bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we load our train data (text8.txt) and parse it into lines. text8 is a clean train data collect from wikipedia."
      ],
      "metadata": {
        "id": "auO16c8E3IC2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZU544k3P-fRr"
      },
      "outputs": [],
      "source": [
        "text = []\n",
        "\n",
        "# Open the text file \n",
        "f = open(\"/content/drive/MyDrive/word-embedding-creation/input/text8\", \"r\")\n",
        "\n",
        "# Extract each line of our text file\n",
        "for line in f:\n",
        "  text.append(line)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries for later use."
      ],
      "metadata": {
        "id": "l5b-ro9KBY1W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dFluvwtRJcq_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, based on the lines from text8 above, we split it into separate words."
      ],
      "metadata": {
        "id": "dgUQlNUwCGCb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QZ3f3O9lJePh"
      },
      "outputs": [],
      "source": [
        "corpus = []\n",
        "# For each line, we split it into words.\n",
        "for i in range(len(text)):\n",
        "  corpus.append(text_to_word_sequence(text[i])) # Same as split\n",
        "  # This is because we want to re-use our code in other corpus that contains multiple lines of text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((corpus[0][:100]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_V9DnTaM6J5",
        "outputId": "ee10ddf5-e969-4f57-bf36-abe0435c1296"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst', 'the', 'term', 'is', 'still', 'used', 'in', 'a', 'pejorative', 'way', 'to', 'describe', 'any', 'act', 'that', 'used', 'violent', 'means', 'to', 'destroy', 'the', 'organization', 'of', 'society', 'it', 'has', 'also', 'been', 'taken', 'up', 'as', 'a', 'positive', 'label', 'by', 'self', 'defined', 'anarchists', 'the', 'word', 'anarchism', 'is', 'derived', 'from', 'the', 'greek', 'without', 'archons', 'ruler', 'chief', 'king', 'anarchism', 'as', 'a', 'political', 'philosophy', 'is', 'the', 'belief', 'that', 'rulers', 'are', 'unnecessary', 'and', 'should', 'be', 'abolished', 'although', 'there', 'are', 'differing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the corpus is very **large**, if we take all vocabulary the appear in the corpus into consideration, our vocab dictionary grows up to 250,000 words. This cause our models to be unreasonably large and take a large amount of time to train. On the other hand, there are many words that appear only several times in the corpus (< 10); thus, we will not be able to obtain good dense representations for it. Therefore, it is essential for use to only choose our corpus to be the 30000 most frequent words."
      ],
      "metadata": {
        "id": "oYVi7lFKCNcv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4MT1AS5gaksp"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "c = Counter(corpus[0])\n",
        "most_30000 = c.most_common(29999)\n",
        "vocab_dictionary = [[i[0] for i in most_30000]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgEkwAoadn84",
        "outputId": "df93ff3d-b642-40a9-8cbc-983cdb7db75e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'of', 'and', 'one', 'in', 'a', 'to', 'zero', 'nine', 'two', 'is', 'as', 'eight', 'for', 's', 'five', 'three', 'was', 'by', 'that', 'four', 'six', 'seven', 'with', 'on', 'are', 'it', 'from', 'or', 'his', 'an', 'be', 'this', 'which', 'at', 'he', 'also', 'not', 'have', 'were', 'has', 'but', 'other', 'their', 'its', 'first', 'they', 'some', 'had', 'all', 'more', 'most', 'can', 'been', 'such', 'many', 'who', 'new', 'used', 'there', 'after', 'when', 'into', 'american', 'time', 'these', 'only', 'see', 'may', 'than', 'world', 'i', 'b', 'would', 'd', 'no', 'however', 'between', 'about', 'over', 'years', 'states', 'people', 'war', 'during', 'united', 'known', 'if', 'called', 'use', 'th', 'system', 'often', 'state', 'so', 'history', 'will', 'up', 'while', 'where']\n"
          ]
        }
      ],
      "source": [
        "print(vocab_dictionary[0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the 30000 words, we generate the index for each word. The first index \n",
        "is reserved for out-of-vocabulary (OOV) word."
      ],
      "metadata": {
        "id": "FG_DgKxgDAPj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJrFfwWYJki_",
        "outputId": "83991546-9fbf-43b5-8c28-b54410495e9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<OOV>': 1, 'the': 2, 'of': 3, 'and': 4, 'one': 5, 'in': 6, 'a': 7, 'to': 8, 'zero': 9, 'nine': 10, 'two': 11, 'is': 12, 'as': 13, 'eight': 14, 'for': 15, 's': 16, 'five': 17, 'three': 18, 'was': 19, 'by': 20, 'that': 21, 'four': 22, 'six': 23, 'seven': 24, 'with': 25, 'on': 26, 'are': 27, 'it': 28, 'from': 29, 'or': 30, 'his': 31, 'an': 32, 'be': 33, 'this': 34, 'which': 35, 'at': 36, 'he': 37, 'also': 38, 'not': 39, 'have': 40, 'were': 41, 'has': 42, 'but': 43, 'other': 44, 'their': 45, 'its': 46, 'first': 47, 'they': 48, 'some': 49, 'had': 50}\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(vocab_dictionary)\n",
        "w2id = tokenizer.word_index\n",
        "print({a: w2id[a] for a in list(w2id.keys())[:50]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqqv3WbX7G39",
        "outputId": "e5d80a44-aba4-43f8-fdea-4aa7aef0e250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        }
      ],
      "source": [
        "print(len(w2id))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K93ro-JHJ3qO"
      },
      "source": [
        "## **2. Preprocess data for Skip-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to preprocess the data for training set. First, we define the vocabulary size and the window size for each word."
      ],
      "metadata": {
        "id": "in2H9T3cENXE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gibTfgThJsXM"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "window_size = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we define a function to generate pairs of words that exist in the corpus based on the window_size and the corpus.\n",
        "\n",
        "Note:\n",
        "For example, we are trying to get training samples with window_size = 2 from sentence:\n",
        "    ***I am a student at Denison.***\n",
        "If the training word is \"student\", then the labels would be \"am\", \"a\", \"at\", \"Denison\"."
      ],
      "metadata": {
        "id": "AJ0nypa-GEEo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZkaywCSF9pi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def generate_pairs(window_size, corpus):\n",
        "\n",
        "    X_cat = []\n",
        "    y_cat = []\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for sent in corpus:                 # This is because we want to re-use our code in other corpus that contains multiple lines of text\n",
        "    tar_i = 0\n",
        "\n",
        "    while tar_i < len(sent):\n",
        "        if sent[tar_i] in w2id:\n",
        "        start = max(0, tar_i-2)\n",
        "        end = min(len(sent)-1, tar_i+2)\n",
        "\n",
        "        labels = sent[start:tar_i] + sent[tar_i+1:end+1]\n",
        "        for label in labels:\n",
        "            if label in w2id:\n",
        "            X.append(sent[tar_i])\n",
        "            y.append(label)\n",
        "            if len(X) >= 100:\n",
        "                # If we store all extracted training data and label in form of python built-in lists, the RAM will be overflowed\n",
        "                X_cat.append(tf.convert_to_tensor(tokenizer.texts_to_sequences(X)))\n",
        "                y_cat.append(tf.convert_to_tensor(tokenizer.texts_to_sequences(y)))\n",
        "                X = []\n",
        "                y = []\n",
        "        tar_i += 1\n",
        "    return tf.concat(X_cat, axis=0) , tf.concat(y_cat, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv8iADeEKl1n"
      },
      "outputs": [],
      "source": [
        "# Generate train data\n",
        "X_train, y_train = generate_pairs(window_size, corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQyLAFOEKm58"
      },
      "outputs": [],
      "source": [
        "# import required libraries for later use\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
        "from tensorflow.keras.backend import mean "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaKN2kSUUlAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee2f38a-02e6-46fa-a2c9-a0e46537b79c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(62834600, 1)\n",
            "(62834600, 1)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape) \n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1eebxSlUmi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab1f587-853d-4c12-dfef-4fc0ea1061f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30001"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69xQ4IHgz7Mk"
      },
      "source": [
        "## **3. Skip_gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we train out model"
      ],
      "metadata": {
        "id": "Yumf64hPHEYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import Input\n",
        "top4_accuracy_metric = tf.metrics.SparseTopKCategoricalAccuracy(k=4, name='top1_acc')\n",
        "\n",
        "# Choose the neutral embedding size\n",
        "embedding_size = 50\n",
        "skip_gram = Sequential()\n",
        "skip_gram.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, embeddings_initializer=\"LecunUniform\"))\n",
        "skip_gram.add(Dense(vocab_size, activation='softmax', use_bias=False, kernel_initializer=\"LecunUniform\"))\n",
        "\n",
        "skip_gram.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=[top4_accuracy_metric])\n",
        "skip_gram.fit(X_train, y_train, epochs=1, verbose=1)\n",
        "skip_gram.save('/content/drive/MyDrive/word-embedding-creation/output/model_full')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYGiGxh8_lfp",
        "outputId": "25df36ab-e307-4862-da2b-00d2b6d06de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  78390/1963582 [>.............................] - ETA: 1:55:40 - loss: 7.2863 - top1_acc: 0.2026"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLGa3TrI0UJ6"
      },
      "outputs": [],
      "source": [
        "  from tensorflow.keras import Input\n",
        "top4_accuracy_metric = tf.metrics.SparseTopKCategoricalAccuracy(k=4, name='top4_acc')\n",
        "mid_size1 = 512\n",
        "mid_size2 = 256\n",
        "embedding_size = 300\n",
        "skip_gram = Sequential()\n",
        "a = Input(shape=(1,vocab_size))\n",
        "skip_gram.add(Embedding(input_dim=vocab_size, output_dim=mid_size1))\n",
        "skip_gram.add(Dense(embedding_size, use_bias=False))\n",
        "skip_gram.add(Dense(mid_size1, use_bias=False))\n",
        "skip_gram.add(Dense(vocab_size, use_bias=False, activation='softmax'))\n",
        "\n",
        "skip_gram.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[top4_accuracy_metric])\n",
        "skip_gram.fit(X_train, y_train, epochs=3, verbose=1)\n",
        "skip_gram.save('/content/drive/MyDrive/word-embedding-creation/output/model_full')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After we train the model, we try to evaluate out model by extracting the vector representation for each word."
      ],
      "metadata": {
        "id": "ddpP1f4NHOac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBgXxyI9lfh0"
      },
      "outputs": [],
      "source": [
        "# function to convert numbers to one hot vectors\n",
        "import numpy as np\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "    temp = np.zeros(vocab_size)\n",
        "    temp[data_point_index] = 1\n",
        "    return temp\n",
        "x_train = [] # input word\n",
        "for data_word in list(w2id.keys())[1:2]:\n",
        "    x_train.append(to_one_hot(w2id[ data_word ], vocab_size))\n",
        "# convert them to numpy arrays\n",
        "x_train = np.asarray(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBYTxzCMp84W",
        "outputId": "903c9b14-d969-42e0-dfb3-60aa4f148189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 30001)\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqgtJZJergoY"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/word-embedding-creation/output/model_full')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rw52a4pKvpJl",
        "outputId": "4609f3ee-c42f-4a5a-f8be-1775e267892e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 300)         9000300   \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 30001)       9000300   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,000,600\n",
            "Trainable params: 18,000,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV2KYaMM0YWq"
      },
      "outputs": [],
      "source": [
        "# Create a new model using part of the layers in our original model.\n",
        "from tensorflow.keras import backend as K\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "keras_function = Model(model.layers[0].input, model.layers[0].output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras_function.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OHMvQA1ovCq",
        "outputId": "92d00b01-76c3-4825-bcc2-6d68b9f8cf7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_input (InputLayer  [(None, None)]           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 300)         9000300   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,000,300\n",
            "Trainable params: 9,000,300\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the vector representation and write the result into result.txt\n",
        "words = list(w2id.keys())\n",
        "result = {}\n",
        "for i in range(1, len(words)):\n",
        "  result[words[i]] = keras_function(np.asarray([w2id[words[i]]])).numpy()[0]\n",
        "f = open(\"/content/drive/MyDrive/word-embedding-creation/result.txt\", \"w\", encoding='utf-8')\n",
        "for i in result:\n",
        "    line = ' '.join([str(i), *[str(j) for j in result[i]]])\n",
        "    line += \"\\n\"\n",
        "    f.write(line)\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "vR4YPiwOaeu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(keras_function(np.asarray([w2id[words[1]]])).numpy()[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxIbry2JEeWu",
        "outputId": "6c0857a6-68ad-4bca-ff9d-d5d482ba79ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxuwKbVu9MBc"
      },
      "source": [
        "## **5. Skip Gram with negative sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We will use this later. Currently, there are several functions that are outdated"
      ],
      "metadata": {
        "id": "iQsnlwSRSWQy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPT9b7MCE_9I"
      },
      "source": [
        "Negative sampling formula:\n",
        "\n",
        "$$P(w _{i}\\hspace{0.001em}) = \\frac{f(w _{i}\\hspace{0.001em})^{3/4}}{\\sum_0^n (f(w _{j}\\hspace{0.001em})^{3/4})}$$\n",
        "\n",
        "$P(w _{i}\\hspace{0.001em})$: probability that $w _{i}\\hspace{0.001em}$ would be selected as a negative sample\n",
        "\n",
        "$f(w _{i}\\hspace{0.001em}$: the number of times that  $w _{i}\\hspace{0.001em}$ appears in the corpus (text8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DsiB83z_XMZ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "int_corpus = []\n",
        "for i in range(len(text)):\n",
        "  text_seq = text_to_word_sequence(text[i])\n",
        "  for word in text_seq:\n",
        "    int_corpus.append(w2id[word])\n",
        "\n",
        "appear_counts = Counter(int_corpus)\n",
        "total_count = len(int_corpus)\n",
        "freqs_dict = {word: count/total_count for word, count in appear_counts.items()}\n",
        "\n",
        "freqs_arr = np.array(sorted(freqs_dict.values(), reverse=True))\n",
        "sampling = tf.convert_to_tensor(freqs_arr**(0.75)/np.sum(freqs_arr**(0.75)))\n",
        "sampling = tf.expand_dims(sampling, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8nKri8GHUrs",
        "outputId": "5f642f2f-0788-4724-bd22-b389dbf32ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1096: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "tf.Tensor([88  2 82 91 74 91 21 26 87 52], shape=(10,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "n_samples = 10\n",
        "negative_samples = tf.compat.v1.multinomial(sampling, y_train.shape[0] * n_samples)\n",
        "negative_samples = tf.reshape(negative_samples, [y_train.shape[0], n_samples])\n",
        "print(negative_samples[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqe-ov9C196Z"
      },
      "outputs": [],
      "source": [
        "class SkipGramNeg(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    \n",
        "    # define embedding layers for input and output words\n",
        "    self.in_embed = Embedding(vocab_size, embedding_size)\n",
        "    self.out_embed = Embedding(vocab_size, embedding_size)\n",
        "  def call(self, inputs, targets, negative_samples):\n",
        "    input_vectors = self.in_embed(inputs)\n",
        "    target_vectors = self.out_embed(targets)\n",
        "    negative_vectors = self.out_embed(negative_samples)\n",
        "\n",
        "    return input_vectors, target_vectors, negative_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thZo6pHyLT7P"
      },
      "outputs": [],
      "source": [
        "from tensorflow import math\n",
        "\n",
        "def negativeSamplingLoss(input_vectors, output_vectors, negative_vectors):\n",
        "    input_vectors =  tf.transpose(input_vectors, perm=(0,2,1))\n",
        "    out_loss = math.log(math.sigmoid(tf.matmul(output_vectors, input_vectors)))\n",
        "    out_loss = tf.squeeze(out_loss)\n",
        "    \n",
        "    # incorrect log-sigmoid loss\n",
        "\n",
        "    negative_loss = math.log(math.sigmoid(tf.matmul(math.negative(negative_vectors), input_vectors)))\n",
        "    negative_loss = math.reduce_sum(tf.squeeze(negative_loss), axis=1)  # sum the losses over the sample of noise vectors\n",
        "\n",
        "    # negate and sum correct and noisy log-sigmoid losses\n",
        "    # return average batch loss\n",
        "    return tf.math.reduce_mean(-(out_loss + negative_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DHwLYTuOfUp"
      },
      "outputs": [],
      "source": [
        "embedding_size = 128\n",
        "neg_skip_gram = SkipGramNeg(vocab_size, embedding_size)\n",
        "loss_fn = negativeSamplingLoss\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oipXqDjVQeCQ",
        "outputId": "a6fd3a04-04aa-429e-b6dd-2dc26daee7b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss: 7.6248\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss: 7.6192\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss: 7.6137\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss: 7.6080\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss: 7.6023\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss: 7.5964\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss: 7.5903\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss: 7.5839\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss: 7.5773\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss: 7.5703\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss: 7.5630\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss: 7.5552\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss: 7.5469\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss: 7.5381\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss: 7.5287\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss: 7.5187\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss: 7.5080\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss: 7.4965\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss: 7.4842\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss: 7.4711\n",
            "\n",
            "Start of epoch 20\n",
            "Training loss: 7.4571\n",
            "\n",
            "Start of epoch 21\n",
            "Training loss: 7.4421\n",
            "\n",
            "Start of epoch 22\n",
            "Training loss: 7.4261\n",
            "\n",
            "Start of epoch 23\n",
            "Training loss: 7.4090\n",
            "\n",
            "Start of epoch 24\n",
            "Training loss: 7.3908\n",
            "\n",
            "Start of epoch 25\n",
            "Training loss: 7.3714\n",
            "\n",
            "Start of epoch 26\n",
            "Training loss: 7.3507\n",
            "\n",
            "Start of epoch 27\n",
            "Training loss: 7.3288\n",
            "\n",
            "Start of epoch 28\n",
            "Training loss: 7.3056\n",
            "\n",
            "Start of epoch 29\n",
            "Training loss: 7.2810\n",
            "\n",
            "Start of epoch 30\n",
            "Training loss: 7.2549\n",
            "\n",
            "Start of epoch 31\n",
            "Training loss: 7.2275\n",
            "\n",
            "Start of epoch 32\n",
            "Training loss: 7.1985\n",
            "\n",
            "Start of epoch 33\n",
            "Training loss: 7.1680\n",
            "\n",
            "Start of epoch 34\n",
            "Training loss: 7.1360\n",
            "\n",
            "Start of epoch 35\n",
            "Training loss: 7.1025\n",
            "\n",
            "Start of epoch 36\n",
            "Training loss: 7.0673\n",
            "\n",
            "Start of epoch 37\n",
            "Training loss: 7.0305\n",
            "\n",
            "Start of epoch 38\n",
            "Training loss: 6.9922\n",
            "\n",
            "Start of epoch 39\n",
            "Training loss: 6.9522\n",
            "\n",
            "Start of epoch 40\n",
            "Training loss: 6.9105\n",
            "\n",
            "Start of epoch 41\n",
            "Training loss: 6.8673\n",
            "\n",
            "Start of epoch 42\n",
            "Training loss: 6.8225\n",
            "\n",
            "Start of epoch 43\n",
            "Training loss: 6.7760\n",
            "\n",
            "Start of epoch 44\n",
            "Training loss: 6.7280\n",
            "\n",
            "Start of epoch 45\n",
            "Training loss: 6.6784\n",
            "\n",
            "Start of epoch 46\n",
            "Training loss: 6.6272\n",
            "\n",
            "Start of epoch 47\n",
            "Training loss: 6.5746\n",
            "\n",
            "Start of epoch 48\n",
            "Training loss: 6.5205\n",
            "\n",
            "Start of epoch 49\n",
            "Training loss: 6.4649\n",
            "\n",
            "Start of epoch 50\n",
            "Training loss: 6.4080\n",
            "\n",
            "Start of epoch 51\n",
            "Training loss: 6.3497\n",
            "\n",
            "Start of epoch 52\n",
            "Training loss: 6.2902\n",
            "\n",
            "Start of epoch 53\n",
            "Training loss: 6.2294\n",
            "\n",
            "Start of epoch 54\n",
            "Training loss: 6.1675\n",
            "\n",
            "Start of epoch 55\n",
            "Training loss: 6.1044\n",
            "\n",
            "Start of epoch 56\n",
            "Training loss: 6.0404\n",
            "\n",
            "Start of epoch 57\n",
            "Training loss: 5.9754\n",
            "\n",
            "Start of epoch 58\n",
            "Training loss: 5.9096\n",
            "\n",
            "Start of epoch 59\n",
            "Training loss: 5.8429\n",
            "\n",
            "Start of epoch 60\n",
            "Training loss: 5.7756\n",
            "\n",
            "Start of epoch 61\n",
            "Training loss: 5.7077\n",
            "\n",
            "Start of epoch 62\n",
            "Training loss: 5.6393\n",
            "\n",
            "Start of epoch 63\n",
            "Training loss: 5.5704\n",
            "\n",
            "Start of epoch 64\n",
            "Training loss: 5.5013\n",
            "\n",
            "Start of epoch 65\n",
            "Training loss: 5.4320\n",
            "\n",
            "Start of epoch 66\n",
            "Training loss: 5.3625\n",
            "\n",
            "Start of epoch 67\n",
            "Training loss: 5.2930\n",
            "\n",
            "Start of epoch 68\n",
            "Training loss: 5.2237\n",
            "\n",
            "Start of epoch 69\n",
            "Training loss: 5.1545\n",
            "\n",
            "Start of epoch 70\n",
            "Training loss: 5.0856\n",
            "\n",
            "Start of epoch 71\n",
            "Training loss: 5.0171\n",
            "\n",
            "Start of epoch 72\n",
            "Training loss: 4.9492\n",
            "\n",
            "Start of epoch 73\n",
            "Training loss: 4.8818\n",
            "\n",
            "Start of epoch 74\n",
            "Training loss: 4.8152\n",
            "\n",
            "Start of epoch 75\n",
            "Training loss: 4.7493\n",
            "\n",
            "Start of epoch 76\n",
            "Training loss: 4.6843\n",
            "\n",
            "Start of epoch 77\n",
            "Training loss: 4.6203\n",
            "\n",
            "Start of epoch 78\n",
            "Training loss: 4.5574\n",
            "\n",
            "Start of epoch 79\n",
            "Training loss: 4.4956\n",
            "\n",
            "Start of epoch 80\n",
            "Training loss: 4.4349\n",
            "\n",
            "Start of epoch 81\n",
            "Training loss: 4.3756\n",
            "\n",
            "Start of epoch 82\n",
            "Training loss: 4.3176\n",
            "\n",
            "Start of epoch 83\n",
            "Training loss: 4.2610\n",
            "\n",
            "Start of epoch 84\n",
            "Training loss: 4.2059\n",
            "\n",
            "Start of epoch 85\n",
            "Training loss: 4.1522\n",
            "\n",
            "Start of epoch 86\n",
            "Training loss: 4.1001\n",
            "\n",
            "Start of epoch 87\n",
            "Training loss: 4.0496\n",
            "\n",
            "Start of epoch 88\n",
            "Training loss: 4.0006\n",
            "\n",
            "Start of epoch 89\n",
            "Training loss: 3.9532\n",
            "\n",
            "Start of epoch 90\n",
            "Training loss: 3.9075\n",
            "\n",
            "Start of epoch 91\n",
            "Training loss: 3.8634\n",
            "\n",
            "Start of epoch 92\n",
            "Training loss: 3.8210\n",
            "\n",
            "Start of epoch 93\n",
            "Training loss: 3.7801\n",
            "\n",
            "Start of epoch 94\n",
            "Training loss: 3.7409\n",
            "\n",
            "Start of epoch 95\n",
            "Training loss: 3.7033\n",
            "\n",
            "Start of epoch 96\n",
            "Training loss: 3.6673\n",
            "\n",
            "Start of epoch 97\n",
            "Training loss: 3.6328\n",
            "\n",
            "Start of epoch 98\n",
            "Training loss: 3.5998\n",
            "\n",
            "Start of epoch 99\n",
            "Training loss: 3.5684\n",
            "\n",
            "Start of epoch 100\n",
            "Training loss: 3.5384\n",
            "\n",
            "Start of epoch 101\n",
            "Training loss: 3.5098\n",
            "\n",
            "Start of epoch 102\n",
            "Training loss: 3.4825\n",
            "\n",
            "Start of epoch 103\n",
            "Training loss: 3.4567\n",
            "\n",
            "Start of epoch 104\n",
            "Training loss: 3.4320\n",
            "\n",
            "Start of epoch 105\n",
            "Training loss: 3.4086\n",
            "\n",
            "Start of epoch 106\n",
            "Training loss: 3.3864\n",
            "\n",
            "Start of epoch 107\n",
            "Training loss: 3.3654\n",
            "\n",
            "Start of epoch 108\n",
            "Training loss: 3.3454\n",
            "\n",
            "Start of epoch 109\n",
            "Training loss: 3.3264\n",
            "\n",
            "Start of epoch 110\n",
            "Training loss: 3.3084\n",
            "\n",
            "Start of epoch 111\n",
            "Training loss: 3.2913\n",
            "\n",
            "Start of epoch 112\n",
            "Training loss: 3.2751\n",
            "\n",
            "Start of epoch 113\n",
            "Training loss: 3.2597\n",
            "\n",
            "Start of epoch 114\n",
            "Training loss: 3.2451\n",
            "\n",
            "Start of epoch 115\n",
            "Training loss: 3.2312\n",
            "\n",
            "Start of epoch 116\n",
            "Training loss: 3.2180\n",
            "\n",
            "Start of epoch 117\n",
            "Training loss: 3.2054\n",
            "\n",
            "Start of epoch 118\n",
            "Training loss: 3.1934\n",
            "\n",
            "Start of epoch 119\n",
            "Training loss: 3.1820\n",
            "\n",
            "Start of epoch 120\n",
            "Training loss: 3.1711\n",
            "\n",
            "Start of epoch 121\n",
            "Training loss: 3.1606\n",
            "\n",
            "Start of epoch 122\n",
            "Training loss: 3.1506\n",
            "\n",
            "Start of epoch 123\n",
            "Training loss: 3.1410\n",
            "\n",
            "Start of epoch 124\n",
            "Training loss: 3.1318\n",
            "\n",
            "Start of epoch 125\n",
            "Training loss: 3.1229\n",
            "\n",
            "Start of epoch 126\n",
            "Training loss: 3.1143\n",
            "\n",
            "Start of epoch 127\n",
            "Training loss: 3.1060\n",
            "\n",
            "Start of epoch 128\n",
            "Training loss: 3.0980\n",
            "\n",
            "Start of epoch 129\n",
            "Training loss: 3.0902\n",
            "\n",
            "Start of epoch 130\n",
            "Training loss: 3.0827\n",
            "\n",
            "Start of epoch 131\n",
            "Training loss: 3.0753\n",
            "\n",
            "Start of epoch 132\n",
            "Training loss: 3.0681\n",
            "\n",
            "Start of epoch 133\n",
            "Training loss: 3.0611\n",
            "\n",
            "Start of epoch 134\n",
            "Training loss: 3.0542\n",
            "\n",
            "Start of epoch 135\n",
            "Training loss: 3.0475\n",
            "\n",
            "Start of epoch 136\n",
            "Training loss: 3.0408\n",
            "\n",
            "Start of epoch 137\n",
            "Training loss: 3.0343\n",
            "\n",
            "Start of epoch 138\n",
            "Training loss: 3.0279\n",
            "\n",
            "Start of epoch 139\n",
            "Training loss: 3.0215\n",
            "\n",
            "Start of epoch 140\n",
            "Training loss: 3.0152\n",
            "\n",
            "Start of epoch 141\n",
            "Training loss: 3.0090\n",
            "\n",
            "Start of epoch 142\n",
            "Training loss: 3.0028\n",
            "\n",
            "Start of epoch 143\n",
            "Training loss: 2.9966\n",
            "\n",
            "Start of epoch 144\n",
            "Training loss: 2.9905\n",
            "\n",
            "Start of epoch 145\n",
            "Training loss: 2.9844\n",
            "\n",
            "Start of epoch 146\n",
            "Training loss: 2.9784\n",
            "\n",
            "Start of epoch 147\n",
            "Training loss: 2.9723\n",
            "\n",
            "Start of epoch 148\n",
            "Training loss: 2.9663\n",
            "\n",
            "Start of epoch 149\n",
            "Training loss: 2.9603\n",
            "\n",
            "Start of epoch 150\n",
            "Training loss: 2.9543\n",
            "\n",
            "Start of epoch 151\n",
            "Training loss: 2.9482\n",
            "\n",
            "Start of epoch 152\n",
            "Training loss: 2.9422\n",
            "\n",
            "Start of epoch 153\n",
            "Training loss: 2.9362\n",
            "\n",
            "Start of epoch 154\n",
            "Training loss: 2.9301\n",
            "\n",
            "Start of epoch 155\n",
            "Training loss: 2.9241\n",
            "\n",
            "Start of epoch 156\n",
            "Training loss: 2.9180\n",
            "\n",
            "Start of epoch 157\n",
            "Training loss: 2.9119\n",
            "\n",
            "Start of epoch 158\n",
            "Training loss: 2.9058\n",
            "\n",
            "Start of epoch 159\n",
            "Training loss: 2.8997\n",
            "\n",
            "Start of epoch 160\n",
            "Training loss: 2.8935\n",
            "\n",
            "Start of epoch 161\n",
            "Training loss: 2.8873\n",
            "\n",
            "Start of epoch 162\n",
            "Training loss: 2.8811\n",
            "\n",
            "Start of epoch 163\n",
            "Training loss: 2.8749\n",
            "\n",
            "Start of epoch 164\n",
            "Training loss: 2.8686\n",
            "\n",
            "Start of epoch 165\n",
            "Training loss: 2.8623\n",
            "\n",
            "Start of epoch 166\n",
            "Training loss: 2.8559\n",
            "\n",
            "Start of epoch 167\n",
            "Training loss: 2.8495\n",
            "\n",
            "Start of epoch 168\n",
            "Training loss: 2.8431\n",
            "\n",
            "Start of epoch 169\n",
            "Training loss: 2.8367\n",
            "\n",
            "Start of epoch 170\n",
            "Training loss: 2.8302\n",
            "\n",
            "Start of epoch 171\n",
            "Training loss: 2.8237\n",
            "\n",
            "Start of epoch 172\n",
            "Training loss: 2.8171\n",
            "\n",
            "Start of epoch 173\n",
            "Training loss: 2.8105\n",
            "\n",
            "Start of epoch 174\n",
            "Training loss: 2.8039\n",
            "\n",
            "Start of epoch 175\n",
            "Training loss: 2.7972\n",
            "\n",
            "Start of epoch 176\n",
            "Training loss: 2.7905\n",
            "\n",
            "Start of epoch 177\n",
            "Training loss: 2.7837\n",
            "\n",
            "Start of epoch 178\n",
            "Training loss: 2.7769\n",
            "\n",
            "Start of epoch 179\n",
            "Training loss: 2.7701\n",
            "\n",
            "Start of epoch 180\n",
            "Training loss: 2.7632\n",
            "\n",
            "Start of epoch 181\n",
            "Training loss: 2.7563\n",
            "\n",
            "Start of epoch 182\n",
            "Training loss: 2.7493\n",
            "\n",
            "Start of epoch 183\n",
            "Training loss: 2.7423\n",
            "\n",
            "Start of epoch 184\n",
            "Training loss: 2.7352\n",
            "\n",
            "Start of epoch 185\n",
            "Training loss: 2.7281\n",
            "\n",
            "Start of epoch 186\n",
            "Training loss: 2.7210\n",
            "\n",
            "Start of epoch 187\n",
            "Training loss: 2.7138\n",
            "\n",
            "Start of epoch 188\n",
            "Training loss: 2.7066\n",
            "\n",
            "Start of epoch 189\n",
            "Training loss: 2.6993\n",
            "\n",
            "Start of epoch 190\n",
            "Training loss: 2.6920\n",
            "\n",
            "Start of epoch 191\n",
            "Training loss: 2.6846\n",
            "\n",
            "Start of epoch 192\n",
            "Training loss: 2.6772\n",
            "\n",
            "Start of epoch 193\n",
            "Training loss: 2.6698\n",
            "\n",
            "Start of epoch 194\n",
            "Training loss: 2.6623\n",
            "\n",
            "Start of epoch 195\n",
            "Training loss: 2.6548\n",
            "\n",
            "Start of epoch 196\n",
            "Training loss: 2.6472\n",
            "\n",
            "Start of epoch 197\n",
            "Training loss: 2.6396\n",
            "\n",
            "Start of epoch 198\n",
            "Training loss: 2.6319\n",
            "\n",
            "Start of epoch 199\n",
            "Training loss: 2.6242\n",
            "\n",
            "Start of epoch 200\n",
            "Training loss: 2.6165\n",
            "\n",
            "Start of epoch 201\n",
            "Training loss: 2.6087\n",
            "\n",
            "Start of epoch 202\n",
            "Training loss: 2.6009\n",
            "\n",
            "Start of epoch 203\n",
            "Training loss: 2.5930\n",
            "\n",
            "Start of epoch 204\n",
            "Training loss: 2.5851\n",
            "\n",
            "Start of epoch 205\n",
            "Training loss: 2.5772\n",
            "\n",
            "Start of epoch 206\n",
            "Training loss: 2.5692\n",
            "\n",
            "Start of epoch 207\n",
            "Training loss: 2.5612\n",
            "\n",
            "Start of epoch 208\n",
            "Training loss: 2.5531\n",
            "\n",
            "Start of epoch 209\n",
            "Training loss: 2.5450\n",
            "\n",
            "Start of epoch 210\n",
            "Training loss: 2.5369\n",
            "\n",
            "Start of epoch 211\n",
            "Training loss: 2.5287\n",
            "\n",
            "Start of epoch 212\n",
            "Training loss: 2.5205\n",
            "\n",
            "Start of epoch 213\n",
            "Training loss: 2.5123\n",
            "\n",
            "Start of epoch 214\n",
            "Training loss: 2.5040\n",
            "\n",
            "Start of epoch 215\n",
            "Training loss: 2.4957\n",
            "\n",
            "Start of epoch 216\n",
            "Training loss: 2.4873\n",
            "\n",
            "Start of epoch 217\n",
            "Training loss: 2.4789\n",
            "\n",
            "Start of epoch 218\n",
            "Training loss: 2.4705\n",
            "\n",
            "Start of epoch 219\n",
            "Training loss: 2.4621\n",
            "\n",
            "Start of epoch 220\n",
            "Training loss: 2.4536\n",
            "\n",
            "Start of epoch 221\n",
            "Training loss: 2.4450\n",
            "\n",
            "Start of epoch 222\n",
            "Training loss: 2.4365\n",
            "\n",
            "Start of epoch 223\n",
            "Training loss: 2.4279\n",
            "\n",
            "Start of epoch 224\n",
            "Training loss: 2.4193\n",
            "\n",
            "Start of epoch 225\n",
            "Training loss: 2.4107\n",
            "\n",
            "Start of epoch 226\n",
            "Training loss: 2.4020\n",
            "\n",
            "Start of epoch 227\n",
            "Training loss: 2.3933\n",
            "\n",
            "Start of epoch 228\n",
            "Training loss: 2.3846\n",
            "\n",
            "Start of epoch 229\n",
            "Training loss: 2.3758\n",
            "\n",
            "Start of epoch 230\n",
            "Training loss: 2.3670\n",
            "\n",
            "Start of epoch 231\n",
            "Training loss: 2.3582\n",
            "\n",
            "Start of epoch 232\n",
            "Training loss: 2.3494\n",
            "\n",
            "Start of epoch 233\n",
            "Training loss: 2.3406\n",
            "\n",
            "Start of epoch 234\n",
            "Training loss: 2.3317\n",
            "\n",
            "Start of epoch 235\n",
            "Training loss: 2.3228\n",
            "\n",
            "Start of epoch 236\n",
            "Training loss: 2.3139\n",
            "\n",
            "Start of epoch 237\n",
            "Training loss: 2.3049\n",
            "\n",
            "Start of epoch 238\n",
            "Training loss: 2.2960\n",
            "\n",
            "Start of epoch 239\n",
            "Training loss: 2.2870\n",
            "\n",
            "Start of epoch 240\n",
            "Training loss: 2.2780\n",
            "\n",
            "Start of epoch 241\n",
            "Training loss: 2.2690\n",
            "\n",
            "Start of epoch 242\n",
            "Training loss: 2.2600\n",
            "\n",
            "Start of epoch 243\n",
            "Training loss: 2.2509\n",
            "\n",
            "Start of epoch 244\n",
            "Training loss: 2.2419\n",
            "\n",
            "Start of epoch 245\n",
            "Training loss: 2.2328\n",
            "\n",
            "Start of epoch 246\n",
            "Training loss: 2.2237\n",
            "\n",
            "Start of epoch 247\n",
            "Training loss: 2.2146\n",
            "\n",
            "Start of epoch 248\n",
            "Training loss: 2.2055\n",
            "\n",
            "Start of epoch 249\n",
            "Training loss: 2.1964\n",
            "\n",
            "Start of epoch 250\n",
            "Training loss: 2.1872\n",
            "\n",
            "Start of epoch 251\n",
            "Training loss: 2.1781\n",
            "\n",
            "Start of epoch 252\n",
            "Training loss: 2.1689\n",
            "\n",
            "Start of epoch 253\n",
            "Training loss: 2.1598\n",
            "\n",
            "Start of epoch 254\n",
            "Training loss: 2.1506\n",
            "\n",
            "Start of epoch 255\n",
            "Training loss: 2.1415\n",
            "\n",
            "Start of epoch 256\n",
            "Training loss: 2.1323\n",
            "\n",
            "Start of epoch 257\n",
            "Training loss: 2.1231\n",
            "\n",
            "Start of epoch 258\n",
            "Training loss: 2.1139\n",
            "\n",
            "Start of epoch 259\n",
            "Training loss: 2.1048\n",
            "\n",
            "Start of epoch 260\n",
            "Training loss: 2.0956\n",
            "\n",
            "Start of epoch 261\n",
            "Training loss: 2.0864\n",
            "\n",
            "Start of epoch 262\n",
            "Training loss: 2.0772\n",
            "\n",
            "Start of epoch 263\n",
            "Training loss: 2.0680\n",
            "\n",
            "Start of epoch 264\n",
            "Training loss: 2.0589\n",
            "\n",
            "Start of epoch 265\n",
            "Training loss: 2.0497\n",
            "\n",
            "Start of epoch 266\n",
            "Training loss: 2.0405\n",
            "\n",
            "Start of epoch 267\n",
            "Training loss: 2.0314\n",
            "\n",
            "Start of epoch 268\n",
            "Training loss: 2.0222\n",
            "\n",
            "Start of epoch 269\n",
            "Training loss: 2.0131\n",
            "\n",
            "Start of epoch 270\n",
            "Training loss: 2.0039\n",
            "\n",
            "Start of epoch 271\n",
            "Training loss: 1.9948\n",
            "\n",
            "Start of epoch 272\n",
            "Training loss: 1.9857\n",
            "\n",
            "Start of epoch 273\n",
            "Training loss: 1.9766\n",
            "\n",
            "Start of epoch 274\n",
            "Training loss: 1.9675\n",
            "\n",
            "Start of epoch 275\n",
            "Training loss: 1.9584\n",
            "\n",
            "Start of epoch 276\n",
            "Training loss: 1.9493\n",
            "\n",
            "Start of epoch 277\n",
            "Training loss: 1.9403\n",
            "\n",
            "Start of epoch 278\n",
            "Training loss: 1.9312\n",
            "\n",
            "Start of epoch 279\n",
            "Training loss: 1.9222\n",
            "\n",
            "Start of epoch 280\n",
            "Training loss: 1.9132\n",
            "\n",
            "Start of epoch 281\n",
            "Training loss: 1.9042\n",
            "\n",
            "Start of epoch 282\n",
            "Training loss: 1.8952\n",
            "\n",
            "Start of epoch 283\n",
            "Training loss: 1.8863\n",
            "\n",
            "Start of epoch 284\n",
            "Training loss: 1.8773\n",
            "\n",
            "Start of epoch 285\n",
            "Training loss: 1.8684\n",
            "\n",
            "Start of epoch 286\n",
            "Training loss: 1.8595\n",
            "\n",
            "Start of epoch 287\n",
            "Training loss: 1.8506\n",
            "\n",
            "Start of epoch 288\n",
            "Training loss: 1.8418\n",
            "\n",
            "Start of epoch 289\n",
            "Training loss: 1.8329\n",
            "\n",
            "Start of epoch 290\n",
            "Training loss: 1.8241\n",
            "\n",
            "Start of epoch 291\n",
            "Training loss: 1.8154\n",
            "\n",
            "Start of epoch 292\n",
            "Training loss: 1.8066\n",
            "\n",
            "Start of epoch 293\n",
            "Training loss: 1.7979\n",
            "\n",
            "Start of epoch 294\n",
            "Training loss: 1.7892\n",
            "\n",
            "Start of epoch 295\n",
            "Training loss: 1.7805\n",
            "\n",
            "Start of epoch 296\n",
            "Training loss: 1.7719\n",
            "\n",
            "Start of epoch 297\n",
            "Training loss: 1.7632\n",
            "\n",
            "Start of epoch 298\n",
            "Training loss: 1.7546\n",
            "\n",
            "Start of epoch 299\n",
            "Training loss: 1.7461\n"
          ]
        }
      ],
      "source": [
        "epochs = 300\n",
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "      # Run the forward pass of the layer.\n",
        "      # The operations that the layer applies\n",
        "      # to its inputs are going to be recorded\n",
        "      # on the GradientTape.\n",
        "      input_vectors, output_vectors, negative_vectors = neg_skip_gram( X_train, y_train, negative_samples)  # Logits for this minibatch\n",
        "      # Compute the loss value for this minibatch.\n",
        "      loss_value = loss_fn(input_vectors, output_vectors, negative_vectors)\n",
        "\n",
        "  # Use the gradient tape to automatically retrieve\n",
        "  # the gradients of the trainable variables with respect to the loss.\n",
        "  grads = tape.gradient(loss_value, neg_skip_gram.trainable_weights)\n",
        "\n",
        "  # Run one step of gradient descent by updating\n",
        "  # the value of the variables to minimize the loss.\n",
        "  optimizer.apply_gradients(zip(grads, neg_skip_gram.trainable_weights))\n",
        "\n",
        "  print(\n",
        "      \"Training loss: %.4f\"\n",
        "      % (float(loss_value))\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkh27URvgcTP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Project1-NLP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}