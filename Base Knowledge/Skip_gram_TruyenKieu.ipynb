{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip-gram TruyenKieu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjXjmdWbJtjK"
      },
      "source": [
        "## **1. The corpus**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56f6KE4p-ONh"
      },
      "source": [
        "# !wget --no-check-certificate \\\n",
        "#     https://gist.githubusercontent.com/khacanh/4c4662fa226db87a4664dfc2f70bc63e/raw/5d8a1d890c73a1e92e6898137db28f3dc0676975/kieu.txt \\\n",
        "#     -O ./kieu.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy540X2eQ9Ai",
        "outputId": "2b239885-4ee2-4c64-97f3-2293aa6bd03a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU544k3P-fRr"
      },
      "source": [
        "text = []\n",
        "f = open(\"/content/drive/MyDrive/word-embedding-creation/input/text8_short\", \"r\")\n",
        "for line in f:\n",
        "  text.append(line)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFluvwtRJcq_"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ3f3O9lJePh"
      },
      "source": [
        "corpus = []\n",
        "for i in range(len(text)):\n",
        "  corpus.append(text_to_word_sequence(text[i]))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJrFfwWYJki_"
      },
      "source": [
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "w2id = tokenizer.word_index"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K93ro-JHJ3qO"
      },
      "source": [
        "## **2. Preprocess data for Skip-gram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gibTfgThJsXM"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "window_size = 2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MfATiBuKEu1"
      },
      "source": [
        "import numpy as np\n",
        "def generate_pairs(window_size, corpus):\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "  for sent in corpus:\n",
        "    tar_i = 0\n",
        "    while tar_i < len(sent):\n",
        "      start = max(0, tar_i-2)\n",
        "      end = min(len(sent)-1, tar_i+2)\n",
        "\n",
        "      labels = sent[start:tar_i] + sent[tar_i+1:end+1]\n",
        "      x = [' '.join([sent[tar_i]] * len(labels))]\n",
        "      labels = [' '.join(labels)]\n",
        "      # print(sent)\n",
        "    #   print(x, \"--->\", labels)\n",
        "      tar_i += 1\n",
        "    #   print(tokenizer.texts_to_sequences(x)[0])\n",
        "    #   print(tokenizer.texts_to_sequences(labels)[0])\n",
        "      X.extend(tokenizer.texts_to_sequences(x)[0])\n",
        "      y.extend(tokenizer.texts_to_sequences(labels)[0])\n",
        "      \n",
        "\n",
        "  return tf.convert_to_tensor(X) , tf.convert_to_tensor(y)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tv8iADeEKl1n"
      },
      "source": [
        "X_train, y_train = generate_pairs(window_size, corpus)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQyLAFOEKm58"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
        "from tensorflow.keras.backend import mean "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPZ6r8uoWYPi"
      },
      "source": [
        "X_train = tf.expand_dims(X_train, axis=1)\n",
        "y_train = tf.expand_dims(y_train, axis=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaKN2kSUUlAC",
        "outputId": "d486da8b-2c14-4ff5-f99f-114c1b42dde4"
      },
      "source": [
        "print(X_train.shape) \n",
        "print(y_train.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(892978, 1)\n",
            "(892978, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1eebxSlUmi5",
        "outputId": "a0e75817-40e8-4c71-964c-3c91cc17eb0f"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20376"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69xQ4IHgz7Mk"
      },
      "source": [
        "## **3. Skip_gram**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34GbAlihUoGr",
        "outputId": "af645132-0208-4005-f5a7-75588e09d1dc"
      },
      "source": [
        "top4_accuracy_metric = tf.metrics.SparseTopKCategoricalAccuracy(k=4, name='top4_acc')\n",
        "embedding_size = 300\n",
        "skip_gram = Sequential()\n",
        "skip_gram.add(Embedding(input_dim=vocab_size, output_dim=embedding_size))\n",
        "skip_gram.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "skip_gram.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[top4_accuracy_metric])\n",
        "skip_gram.fit(X_train, y_train, epochs=30, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "27906/27906 [==============================] - 364s 13ms/step - loss: 7.2242 - top4_acc: 0.1934\n",
            "Epoch 2/30\n",
            " 1614/27906 [>.............................] - ETA: 5:43 - loss: 6.5645 - top4_acc: 0.2201"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXy7HHKFzvsH"
      },
      "source": [
        "## **4. Some intuition 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1OZWQSS0H1W"
      },
      "source": [
        "### *4.1. top4 accuracy*\n",
        "\n",
        "Thay vì sử dụng metric accuracy như notebook CBOW, em sử dụng TopKAccuracy vì mỗi một input vào có thể có `2*window_size` outputs, vì vậy nếu target của một sample chỉ cần nằm trong top `2*window_size` có softmax score cao nhất thì đã có thể đánh giá model đã perform tốt trên sample đó. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ifv8w9J1ws8"
      },
      "source": [
        "### *4.2. Dense layer as another Embedding:*\n",
        "\n",
        "Số lượng parameters của lớp `Dense` trong model được huấn luyện phía trên là `embedding_dim * vocab_size`. Có thể nói số lượng weights trong lớp `Embedding` và lớp `Dense` là giống nhau. Tuy nhiên, cách hoạt động ngược nhau: Lớp `Embedding` nhận vào một `one_hot vector` và cho ra embedding của từ tương ứng với `one_hot` đó; còn Dense nhận vào embedding và được train để cho ra `one_hot vector` (tất nhiên nó sẽ không thể cho ra một `one_hot vector` :\">)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGnLurFQ4DuM"
      },
      "source": [
        "### *4.3. The inefficiency of learning process:*\n",
        "\n",
        "Từ cross_entropy_loss function: $L=−y⋅log(y_{predicted}\\hspace{0.001em})$\n",
        "\n",
        "Ta có thể thấy, learning process chỉ tập trung vào target word (từ có giá trị tương ứng trong target one_hot vector bằng 1) và hoàn toàn bỏ qua tất cả những embedding của negative words. Điều này dường như rất bất hợp lí, bởi vì để compute được logits vector để đưa vào cross_entropy_loss thì cũng phải tính toán phép nhân một in_embedding_vector với out_embedding_vector**s** của toàn bộ vocabulary. Hay nói cách khác, nếu chỉ đơn thuần train với loss function là cross_entropy_loss thì mỗi lần backpropagation, chúng ta chỉ update ít hơn 1% (1/118) weights của lớp Dense. It's totally ineffecient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxuwKbVu9MBc"
      },
      "source": [
        "## **5. Skip Gram with negative sampling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPT9b7MCE_9I"
      },
      "source": [
        "Negative sampling formula:\n",
        "\n",
        "$$P(w _{i}\\hspace{0.001em}) = \\frac{f(w _{i}\\hspace{0.001em})^{3/4}}{\\sum_0^n (f(w _{j}\\hspace{0.001em})^{3/4})}$$\n",
        "\n",
        "$P(w _{i}\\hspace{0.001em})$: xác suất từ $w _{i}\\hspace{0.001em}$ sẽ được chọn làm negative words\n",
        "\n",
        "$f(w _{i}\\hspace{0.001em}$: số lần $w _{i}\\hspace{0.001em}$ suất hiện trong corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DsiB83z_XMZ"
      },
      "source": [
        "from collections import Counter\n",
        "int_corpus = []\n",
        "for i in range(len(text)):\n",
        "  text_seq = text_to_word_sequence(text[i])\n",
        "  for word in text_seq:\n",
        "    int_corpus.append(w2id[word])\n",
        "\n",
        "appear_counts = Counter(int_corpus)\n",
        "total_count = len(int_corpus)\n",
        "freqs_dict = {word: count/total_count for word, count in appear_counts.items()}\n",
        "\n",
        "freqs_arr = np.array(sorted(freqs_dict.values(), reverse=True))\n",
        "sampling = tf.convert_to_tensor(freqs_arr**(0.75)/np.sum(freqs_arr**(0.75)))\n",
        "sampling = tf.expand_dims(sampling, axis=0)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sampling.shape)"
      ],
      "metadata": {
        "id": "bI7oufVjbhDr",
        "outputId": "9ee424db-2251-496d-86de-f562a290bd73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 20374)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "h8nKri8GHUrs",
        "outputId": "0f1bdebc-e5ef-458a-bf88-a978897f0917"
      },
      "source": [
        "n_samples = 10\n",
        "negative_samples = tf.compat.v1.multinomial(sampling, y_train.shape[0] * n_samples)\n",
        "negative_samples = tf.reshape(negative_samples, [y_train.shape[0], n_samples])\n",
        "print(negative_samples[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9b190dc30a03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnegative_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnegative_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,8929780,20374] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Multinomial]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqe-ov9C196Z"
      },
      "source": [
        "class SkipGramNeg(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_size):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    \n",
        "    # define embedding layers for input and output words\n",
        "    self.in_embed = Embedding(vocab_size, embedding_size)\n",
        "    self.out_embed = Embedding(vocab_size, embedding_size)\n",
        "  def call(self, inputs, targets, negative_samples):\n",
        "    input_vectors = self.in_embed(inputs)\n",
        "    target_vectors = self.out_embed(targets)\n",
        "    negative_vectors = self.out_embed(negative_samples)\n",
        "\n",
        "    return input_vectors, target_vectors, negative_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thZo6pHyLT7P"
      },
      "source": [
        "from tensorflow import math\n",
        "\n",
        "def negativeSamplingLoss(input_vectors, output_vectors, negative_vectors):\n",
        "    input_vectors =  tf.transpose(input_vectors, perm=(0,2,1))\n",
        "    out_loss = math.log(math.sigmoid(tf.matmul(output_vectors, input_vectors)))\n",
        "    out_loss = tf.squeeze(out_loss)\n",
        "    \n",
        "    # incorrect log-sigmoid loss\n",
        "\n",
        "    negative_loss = math.log(math.sigmoid(tf.matmul(math.negative(negative_vectors), input_vectors)))\n",
        "    negative_loss = math.reduce_sum(tf.squeeze(negative_loss), axis=1)  # sum the losses over the sample of noise vectors\n",
        "\n",
        "    # negate and sum correct and noisy log-sigmoid losses\n",
        "    # return average batch loss\n",
        "    return tf.math.reduce_mean(-(out_loss + negative_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DHwLYTuOfUp"
      },
      "source": [
        "embedding_size = 128\n",
        "neg_skip_gram = SkipGramNeg(vocab_size, embedding_size)\n",
        "loss_fn = negativeSamplingLoss\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oipXqDjVQeCQ",
        "outputId": "a6fd3a04-04aa-429e-b6dd-2dc26daee7b4"
      },
      "source": [
        "epochs = 300\n",
        "for epoch in range(epochs):\n",
        "  print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "      # Run the forward pass of the layer.\n",
        "      # The operations that the layer applies\n",
        "      # to its inputs are going to be recorded\n",
        "      # on the GradientTape.\n",
        "      input_vectors, output_vectors, negative_vectors = neg_skip_gram( X_train, y_train, negative_samples)  # Logits for this minibatch\n",
        "      # Compute the loss value for this minibatch.\n",
        "      loss_value = loss_fn(input_vectors, output_vectors, negative_vectors)\n",
        "\n",
        "  # Use the gradient tape to automatically retrieve\n",
        "  # the gradients of the trainable variables with respect to the loss.\n",
        "  grads = tape.gradient(loss_value, neg_skip_gram.trainable_weights)\n",
        "\n",
        "  # Run one step of gradient descent by updating\n",
        "  # the value of the variables to minimize the loss.\n",
        "  optimizer.apply_gradients(zip(grads, neg_skip_gram.trainable_weights))\n",
        "\n",
        "  print(\n",
        "      \"Training loss: %.4f\"\n",
        "      % (float(loss_value))\n",
        "  )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss: 7.6248\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss: 7.6192\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss: 7.6137\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss: 7.6080\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss: 7.6023\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss: 7.5964\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss: 7.5903\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss: 7.5839\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss: 7.5773\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss: 7.5703\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss: 7.5630\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss: 7.5552\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss: 7.5469\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss: 7.5381\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss: 7.5287\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss: 7.5187\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss: 7.5080\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss: 7.4965\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss: 7.4842\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss: 7.4711\n",
            "\n",
            "Start of epoch 20\n",
            "Training loss: 7.4571\n",
            "\n",
            "Start of epoch 21\n",
            "Training loss: 7.4421\n",
            "\n",
            "Start of epoch 22\n",
            "Training loss: 7.4261\n",
            "\n",
            "Start of epoch 23\n",
            "Training loss: 7.4090\n",
            "\n",
            "Start of epoch 24\n",
            "Training loss: 7.3908\n",
            "\n",
            "Start of epoch 25\n",
            "Training loss: 7.3714\n",
            "\n",
            "Start of epoch 26\n",
            "Training loss: 7.3507\n",
            "\n",
            "Start of epoch 27\n",
            "Training loss: 7.3288\n",
            "\n",
            "Start of epoch 28\n",
            "Training loss: 7.3056\n",
            "\n",
            "Start of epoch 29\n",
            "Training loss: 7.2810\n",
            "\n",
            "Start of epoch 30\n",
            "Training loss: 7.2549\n",
            "\n",
            "Start of epoch 31\n",
            "Training loss: 7.2275\n",
            "\n",
            "Start of epoch 32\n",
            "Training loss: 7.1985\n",
            "\n",
            "Start of epoch 33\n",
            "Training loss: 7.1680\n",
            "\n",
            "Start of epoch 34\n",
            "Training loss: 7.1360\n",
            "\n",
            "Start of epoch 35\n",
            "Training loss: 7.1025\n",
            "\n",
            "Start of epoch 36\n",
            "Training loss: 7.0673\n",
            "\n",
            "Start of epoch 37\n",
            "Training loss: 7.0305\n",
            "\n",
            "Start of epoch 38\n",
            "Training loss: 6.9922\n",
            "\n",
            "Start of epoch 39\n",
            "Training loss: 6.9522\n",
            "\n",
            "Start of epoch 40\n",
            "Training loss: 6.9105\n",
            "\n",
            "Start of epoch 41\n",
            "Training loss: 6.8673\n",
            "\n",
            "Start of epoch 42\n",
            "Training loss: 6.8225\n",
            "\n",
            "Start of epoch 43\n",
            "Training loss: 6.7760\n",
            "\n",
            "Start of epoch 44\n",
            "Training loss: 6.7280\n",
            "\n",
            "Start of epoch 45\n",
            "Training loss: 6.6784\n",
            "\n",
            "Start of epoch 46\n",
            "Training loss: 6.6272\n",
            "\n",
            "Start of epoch 47\n",
            "Training loss: 6.5746\n",
            "\n",
            "Start of epoch 48\n",
            "Training loss: 6.5205\n",
            "\n",
            "Start of epoch 49\n",
            "Training loss: 6.4649\n",
            "\n",
            "Start of epoch 50\n",
            "Training loss: 6.4080\n",
            "\n",
            "Start of epoch 51\n",
            "Training loss: 6.3497\n",
            "\n",
            "Start of epoch 52\n",
            "Training loss: 6.2902\n",
            "\n",
            "Start of epoch 53\n",
            "Training loss: 6.2294\n",
            "\n",
            "Start of epoch 54\n",
            "Training loss: 6.1675\n",
            "\n",
            "Start of epoch 55\n",
            "Training loss: 6.1044\n",
            "\n",
            "Start of epoch 56\n",
            "Training loss: 6.0404\n",
            "\n",
            "Start of epoch 57\n",
            "Training loss: 5.9754\n",
            "\n",
            "Start of epoch 58\n",
            "Training loss: 5.9096\n",
            "\n",
            "Start of epoch 59\n",
            "Training loss: 5.8429\n",
            "\n",
            "Start of epoch 60\n",
            "Training loss: 5.7756\n",
            "\n",
            "Start of epoch 61\n",
            "Training loss: 5.7077\n",
            "\n",
            "Start of epoch 62\n",
            "Training loss: 5.6393\n",
            "\n",
            "Start of epoch 63\n",
            "Training loss: 5.5704\n",
            "\n",
            "Start of epoch 64\n",
            "Training loss: 5.5013\n",
            "\n",
            "Start of epoch 65\n",
            "Training loss: 5.4320\n",
            "\n",
            "Start of epoch 66\n",
            "Training loss: 5.3625\n",
            "\n",
            "Start of epoch 67\n",
            "Training loss: 5.2930\n",
            "\n",
            "Start of epoch 68\n",
            "Training loss: 5.2237\n",
            "\n",
            "Start of epoch 69\n",
            "Training loss: 5.1545\n",
            "\n",
            "Start of epoch 70\n",
            "Training loss: 5.0856\n",
            "\n",
            "Start of epoch 71\n",
            "Training loss: 5.0171\n",
            "\n",
            "Start of epoch 72\n",
            "Training loss: 4.9492\n",
            "\n",
            "Start of epoch 73\n",
            "Training loss: 4.8818\n",
            "\n",
            "Start of epoch 74\n",
            "Training loss: 4.8152\n",
            "\n",
            "Start of epoch 75\n",
            "Training loss: 4.7493\n",
            "\n",
            "Start of epoch 76\n",
            "Training loss: 4.6843\n",
            "\n",
            "Start of epoch 77\n",
            "Training loss: 4.6203\n",
            "\n",
            "Start of epoch 78\n",
            "Training loss: 4.5574\n",
            "\n",
            "Start of epoch 79\n",
            "Training loss: 4.4956\n",
            "\n",
            "Start of epoch 80\n",
            "Training loss: 4.4349\n",
            "\n",
            "Start of epoch 81\n",
            "Training loss: 4.3756\n",
            "\n",
            "Start of epoch 82\n",
            "Training loss: 4.3176\n",
            "\n",
            "Start of epoch 83\n",
            "Training loss: 4.2610\n",
            "\n",
            "Start of epoch 84\n",
            "Training loss: 4.2059\n",
            "\n",
            "Start of epoch 85\n",
            "Training loss: 4.1522\n",
            "\n",
            "Start of epoch 86\n",
            "Training loss: 4.1001\n",
            "\n",
            "Start of epoch 87\n",
            "Training loss: 4.0496\n",
            "\n",
            "Start of epoch 88\n",
            "Training loss: 4.0006\n",
            "\n",
            "Start of epoch 89\n",
            "Training loss: 3.9532\n",
            "\n",
            "Start of epoch 90\n",
            "Training loss: 3.9075\n",
            "\n",
            "Start of epoch 91\n",
            "Training loss: 3.8634\n",
            "\n",
            "Start of epoch 92\n",
            "Training loss: 3.8210\n",
            "\n",
            "Start of epoch 93\n",
            "Training loss: 3.7801\n",
            "\n",
            "Start of epoch 94\n",
            "Training loss: 3.7409\n",
            "\n",
            "Start of epoch 95\n",
            "Training loss: 3.7033\n",
            "\n",
            "Start of epoch 96\n",
            "Training loss: 3.6673\n",
            "\n",
            "Start of epoch 97\n",
            "Training loss: 3.6328\n",
            "\n",
            "Start of epoch 98\n",
            "Training loss: 3.5998\n",
            "\n",
            "Start of epoch 99\n",
            "Training loss: 3.5684\n",
            "\n",
            "Start of epoch 100\n",
            "Training loss: 3.5384\n",
            "\n",
            "Start of epoch 101\n",
            "Training loss: 3.5098\n",
            "\n",
            "Start of epoch 102\n",
            "Training loss: 3.4825\n",
            "\n",
            "Start of epoch 103\n",
            "Training loss: 3.4567\n",
            "\n",
            "Start of epoch 104\n",
            "Training loss: 3.4320\n",
            "\n",
            "Start of epoch 105\n",
            "Training loss: 3.4086\n",
            "\n",
            "Start of epoch 106\n",
            "Training loss: 3.3864\n",
            "\n",
            "Start of epoch 107\n",
            "Training loss: 3.3654\n",
            "\n",
            "Start of epoch 108\n",
            "Training loss: 3.3454\n",
            "\n",
            "Start of epoch 109\n",
            "Training loss: 3.3264\n",
            "\n",
            "Start of epoch 110\n",
            "Training loss: 3.3084\n",
            "\n",
            "Start of epoch 111\n",
            "Training loss: 3.2913\n",
            "\n",
            "Start of epoch 112\n",
            "Training loss: 3.2751\n",
            "\n",
            "Start of epoch 113\n",
            "Training loss: 3.2597\n",
            "\n",
            "Start of epoch 114\n",
            "Training loss: 3.2451\n",
            "\n",
            "Start of epoch 115\n",
            "Training loss: 3.2312\n",
            "\n",
            "Start of epoch 116\n",
            "Training loss: 3.2180\n",
            "\n",
            "Start of epoch 117\n",
            "Training loss: 3.2054\n",
            "\n",
            "Start of epoch 118\n",
            "Training loss: 3.1934\n",
            "\n",
            "Start of epoch 119\n",
            "Training loss: 3.1820\n",
            "\n",
            "Start of epoch 120\n",
            "Training loss: 3.1711\n",
            "\n",
            "Start of epoch 121\n",
            "Training loss: 3.1606\n",
            "\n",
            "Start of epoch 122\n",
            "Training loss: 3.1506\n",
            "\n",
            "Start of epoch 123\n",
            "Training loss: 3.1410\n",
            "\n",
            "Start of epoch 124\n",
            "Training loss: 3.1318\n",
            "\n",
            "Start of epoch 125\n",
            "Training loss: 3.1229\n",
            "\n",
            "Start of epoch 126\n",
            "Training loss: 3.1143\n",
            "\n",
            "Start of epoch 127\n",
            "Training loss: 3.1060\n",
            "\n",
            "Start of epoch 128\n",
            "Training loss: 3.0980\n",
            "\n",
            "Start of epoch 129\n",
            "Training loss: 3.0902\n",
            "\n",
            "Start of epoch 130\n",
            "Training loss: 3.0827\n",
            "\n",
            "Start of epoch 131\n",
            "Training loss: 3.0753\n",
            "\n",
            "Start of epoch 132\n",
            "Training loss: 3.0681\n",
            "\n",
            "Start of epoch 133\n",
            "Training loss: 3.0611\n",
            "\n",
            "Start of epoch 134\n",
            "Training loss: 3.0542\n",
            "\n",
            "Start of epoch 135\n",
            "Training loss: 3.0475\n",
            "\n",
            "Start of epoch 136\n",
            "Training loss: 3.0408\n",
            "\n",
            "Start of epoch 137\n",
            "Training loss: 3.0343\n",
            "\n",
            "Start of epoch 138\n",
            "Training loss: 3.0279\n",
            "\n",
            "Start of epoch 139\n",
            "Training loss: 3.0215\n",
            "\n",
            "Start of epoch 140\n",
            "Training loss: 3.0152\n",
            "\n",
            "Start of epoch 141\n",
            "Training loss: 3.0090\n",
            "\n",
            "Start of epoch 142\n",
            "Training loss: 3.0028\n",
            "\n",
            "Start of epoch 143\n",
            "Training loss: 2.9966\n",
            "\n",
            "Start of epoch 144\n",
            "Training loss: 2.9905\n",
            "\n",
            "Start of epoch 145\n",
            "Training loss: 2.9844\n",
            "\n",
            "Start of epoch 146\n",
            "Training loss: 2.9784\n",
            "\n",
            "Start of epoch 147\n",
            "Training loss: 2.9723\n",
            "\n",
            "Start of epoch 148\n",
            "Training loss: 2.9663\n",
            "\n",
            "Start of epoch 149\n",
            "Training loss: 2.9603\n",
            "\n",
            "Start of epoch 150\n",
            "Training loss: 2.9543\n",
            "\n",
            "Start of epoch 151\n",
            "Training loss: 2.9482\n",
            "\n",
            "Start of epoch 152\n",
            "Training loss: 2.9422\n",
            "\n",
            "Start of epoch 153\n",
            "Training loss: 2.9362\n",
            "\n",
            "Start of epoch 154\n",
            "Training loss: 2.9301\n",
            "\n",
            "Start of epoch 155\n",
            "Training loss: 2.9241\n",
            "\n",
            "Start of epoch 156\n",
            "Training loss: 2.9180\n",
            "\n",
            "Start of epoch 157\n",
            "Training loss: 2.9119\n",
            "\n",
            "Start of epoch 158\n",
            "Training loss: 2.9058\n",
            "\n",
            "Start of epoch 159\n",
            "Training loss: 2.8997\n",
            "\n",
            "Start of epoch 160\n",
            "Training loss: 2.8935\n",
            "\n",
            "Start of epoch 161\n",
            "Training loss: 2.8873\n",
            "\n",
            "Start of epoch 162\n",
            "Training loss: 2.8811\n",
            "\n",
            "Start of epoch 163\n",
            "Training loss: 2.8749\n",
            "\n",
            "Start of epoch 164\n",
            "Training loss: 2.8686\n",
            "\n",
            "Start of epoch 165\n",
            "Training loss: 2.8623\n",
            "\n",
            "Start of epoch 166\n",
            "Training loss: 2.8559\n",
            "\n",
            "Start of epoch 167\n",
            "Training loss: 2.8495\n",
            "\n",
            "Start of epoch 168\n",
            "Training loss: 2.8431\n",
            "\n",
            "Start of epoch 169\n",
            "Training loss: 2.8367\n",
            "\n",
            "Start of epoch 170\n",
            "Training loss: 2.8302\n",
            "\n",
            "Start of epoch 171\n",
            "Training loss: 2.8237\n",
            "\n",
            "Start of epoch 172\n",
            "Training loss: 2.8171\n",
            "\n",
            "Start of epoch 173\n",
            "Training loss: 2.8105\n",
            "\n",
            "Start of epoch 174\n",
            "Training loss: 2.8039\n",
            "\n",
            "Start of epoch 175\n",
            "Training loss: 2.7972\n",
            "\n",
            "Start of epoch 176\n",
            "Training loss: 2.7905\n",
            "\n",
            "Start of epoch 177\n",
            "Training loss: 2.7837\n",
            "\n",
            "Start of epoch 178\n",
            "Training loss: 2.7769\n",
            "\n",
            "Start of epoch 179\n",
            "Training loss: 2.7701\n",
            "\n",
            "Start of epoch 180\n",
            "Training loss: 2.7632\n",
            "\n",
            "Start of epoch 181\n",
            "Training loss: 2.7563\n",
            "\n",
            "Start of epoch 182\n",
            "Training loss: 2.7493\n",
            "\n",
            "Start of epoch 183\n",
            "Training loss: 2.7423\n",
            "\n",
            "Start of epoch 184\n",
            "Training loss: 2.7352\n",
            "\n",
            "Start of epoch 185\n",
            "Training loss: 2.7281\n",
            "\n",
            "Start of epoch 186\n",
            "Training loss: 2.7210\n",
            "\n",
            "Start of epoch 187\n",
            "Training loss: 2.7138\n",
            "\n",
            "Start of epoch 188\n",
            "Training loss: 2.7066\n",
            "\n",
            "Start of epoch 189\n",
            "Training loss: 2.6993\n",
            "\n",
            "Start of epoch 190\n",
            "Training loss: 2.6920\n",
            "\n",
            "Start of epoch 191\n",
            "Training loss: 2.6846\n",
            "\n",
            "Start of epoch 192\n",
            "Training loss: 2.6772\n",
            "\n",
            "Start of epoch 193\n",
            "Training loss: 2.6698\n",
            "\n",
            "Start of epoch 194\n",
            "Training loss: 2.6623\n",
            "\n",
            "Start of epoch 195\n",
            "Training loss: 2.6548\n",
            "\n",
            "Start of epoch 196\n",
            "Training loss: 2.6472\n",
            "\n",
            "Start of epoch 197\n",
            "Training loss: 2.6396\n",
            "\n",
            "Start of epoch 198\n",
            "Training loss: 2.6319\n",
            "\n",
            "Start of epoch 199\n",
            "Training loss: 2.6242\n",
            "\n",
            "Start of epoch 200\n",
            "Training loss: 2.6165\n",
            "\n",
            "Start of epoch 201\n",
            "Training loss: 2.6087\n",
            "\n",
            "Start of epoch 202\n",
            "Training loss: 2.6009\n",
            "\n",
            "Start of epoch 203\n",
            "Training loss: 2.5930\n",
            "\n",
            "Start of epoch 204\n",
            "Training loss: 2.5851\n",
            "\n",
            "Start of epoch 205\n",
            "Training loss: 2.5772\n",
            "\n",
            "Start of epoch 206\n",
            "Training loss: 2.5692\n",
            "\n",
            "Start of epoch 207\n",
            "Training loss: 2.5612\n",
            "\n",
            "Start of epoch 208\n",
            "Training loss: 2.5531\n",
            "\n",
            "Start of epoch 209\n",
            "Training loss: 2.5450\n",
            "\n",
            "Start of epoch 210\n",
            "Training loss: 2.5369\n",
            "\n",
            "Start of epoch 211\n",
            "Training loss: 2.5287\n",
            "\n",
            "Start of epoch 212\n",
            "Training loss: 2.5205\n",
            "\n",
            "Start of epoch 213\n",
            "Training loss: 2.5123\n",
            "\n",
            "Start of epoch 214\n",
            "Training loss: 2.5040\n",
            "\n",
            "Start of epoch 215\n",
            "Training loss: 2.4957\n",
            "\n",
            "Start of epoch 216\n",
            "Training loss: 2.4873\n",
            "\n",
            "Start of epoch 217\n",
            "Training loss: 2.4789\n",
            "\n",
            "Start of epoch 218\n",
            "Training loss: 2.4705\n",
            "\n",
            "Start of epoch 219\n",
            "Training loss: 2.4621\n",
            "\n",
            "Start of epoch 220\n",
            "Training loss: 2.4536\n",
            "\n",
            "Start of epoch 221\n",
            "Training loss: 2.4450\n",
            "\n",
            "Start of epoch 222\n",
            "Training loss: 2.4365\n",
            "\n",
            "Start of epoch 223\n",
            "Training loss: 2.4279\n",
            "\n",
            "Start of epoch 224\n",
            "Training loss: 2.4193\n",
            "\n",
            "Start of epoch 225\n",
            "Training loss: 2.4107\n",
            "\n",
            "Start of epoch 226\n",
            "Training loss: 2.4020\n",
            "\n",
            "Start of epoch 227\n",
            "Training loss: 2.3933\n",
            "\n",
            "Start of epoch 228\n",
            "Training loss: 2.3846\n",
            "\n",
            "Start of epoch 229\n",
            "Training loss: 2.3758\n",
            "\n",
            "Start of epoch 230\n",
            "Training loss: 2.3670\n",
            "\n",
            "Start of epoch 231\n",
            "Training loss: 2.3582\n",
            "\n",
            "Start of epoch 232\n",
            "Training loss: 2.3494\n",
            "\n",
            "Start of epoch 233\n",
            "Training loss: 2.3406\n",
            "\n",
            "Start of epoch 234\n",
            "Training loss: 2.3317\n",
            "\n",
            "Start of epoch 235\n",
            "Training loss: 2.3228\n",
            "\n",
            "Start of epoch 236\n",
            "Training loss: 2.3139\n",
            "\n",
            "Start of epoch 237\n",
            "Training loss: 2.3049\n",
            "\n",
            "Start of epoch 238\n",
            "Training loss: 2.2960\n",
            "\n",
            "Start of epoch 239\n",
            "Training loss: 2.2870\n",
            "\n",
            "Start of epoch 240\n",
            "Training loss: 2.2780\n",
            "\n",
            "Start of epoch 241\n",
            "Training loss: 2.2690\n",
            "\n",
            "Start of epoch 242\n",
            "Training loss: 2.2600\n",
            "\n",
            "Start of epoch 243\n",
            "Training loss: 2.2509\n",
            "\n",
            "Start of epoch 244\n",
            "Training loss: 2.2419\n",
            "\n",
            "Start of epoch 245\n",
            "Training loss: 2.2328\n",
            "\n",
            "Start of epoch 246\n",
            "Training loss: 2.2237\n",
            "\n",
            "Start of epoch 247\n",
            "Training loss: 2.2146\n",
            "\n",
            "Start of epoch 248\n",
            "Training loss: 2.2055\n",
            "\n",
            "Start of epoch 249\n",
            "Training loss: 2.1964\n",
            "\n",
            "Start of epoch 250\n",
            "Training loss: 2.1872\n",
            "\n",
            "Start of epoch 251\n",
            "Training loss: 2.1781\n",
            "\n",
            "Start of epoch 252\n",
            "Training loss: 2.1689\n",
            "\n",
            "Start of epoch 253\n",
            "Training loss: 2.1598\n",
            "\n",
            "Start of epoch 254\n",
            "Training loss: 2.1506\n",
            "\n",
            "Start of epoch 255\n",
            "Training loss: 2.1415\n",
            "\n",
            "Start of epoch 256\n",
            "Training loss: 2.1323\n",
            "\n",
            "Start of epoch 257\n",
            "Training loss: 2.1231\n",
            "\n",
            "Start of epoch 258\n",
            "Training loss: 2.1139\n",
            "\n",
            "Start of epoch 259\n",
            "Training loss: 2.1048\n",
            "\n",
            "Start of epoch 260\n",
            "Training loss: 2.0956\n",
            "\n",
            "Start of epoch 261\n",
            "Training loss: 2.0864\n",
            "\n",
            "Start of epoch 262\n",
            "Training loss: 2.0772\n",
            "\n",
            "Start of epoch 263\n",
            "Training loss: 2.0680\n",
            "\n",
            "Start of epoch 264\n",
            "Training loss: 2.0589\n",
            "\n",
            "Start of epoch 265\n",
            "Training loss: 2.0497\n",
            "\n",
            "Start of epoch 266\n",
            "Training loss: 2.0405\n",
            "\n",
            "Start of epoch 267\n",
            "Training loss: 2.0314\n",
            "\n",
            "Start of epoch 268\n",
            "Training loss: 2.0222\n",
            "\n",
            "Start of epoch 269\n",
            "Training loss: 2.0131\n",
            "\n",
            "Start of epoch 270\n",
            "Training loss: 2.0039\n",
            "\n",
            "Start of epoch 271\n",
            "Training loss: 1.9948\n",
            "\n",
            "Start of epoch 272\n",
            "Training loss: 1.9857\n",
            "\n",
            "Start of epoch 273\n",
            "Training loss: 1.9766\n",
            "\n",
            "Start of epoch 274\n",
            "Training loss: 1.9675\n",
            "\n",
            "Start of epoch 275\n",
            "Training loss: 1.9584\n",
            "\n",
            "Start of epoch 276\n",
            "Training loss: 1.9493\n",
            "\n",
            "Start of epoch 277\n",
            "Training loss: 1.9403\n",
            "\n",
            "Start of epoch 278\n",
            "Training loss: 1.9312\n",
            "\n",
            "Start of epoch 279\n",
            "Training loss: 1.9222\n",
            "\n",
            "Start of epoch 280\n",
            "Training loss: 1.9132\n",
            "\n",
            "Start of epoch 281\n",
            "Training loss: 1.9042\n",
            "\n",
            "Start of epoch 282\n",
            "Training loss: 1.8952\n",
            "\n",
            "Start of epoch 283\n",
            "Training loss: 1.8863\n",
            "\n",
            "Start of epoch 284\n",
            "Training loss: 1.8773\n",
            "\n",
            "Start of epoch 285\n",
            "Training loss: 1.8684\n",
            "\n",
            "Start of epoch 286\n",
            "Training loss: 1.8595\n",
            "\n",
            "Start of epoch 287\n",
            "Training loss: 1.8506\n",
            "\n",
            "Start of epoch 288\n",
            "Training loss: 1.8418\n",
            "\n",
            "Start of epoch 289\n",
            "Training loss: 1.8329\n",
            "\n",
            "Start of epoch 290\n",
            "Training loss: 1.8241\n",
            "\n",
            "Start of epoch 291\n",
            "Training loss: 1.8154\n",
            "\n",
            "Start of epoch 292\n",
            "Training loss: 1.8066\n",
            "\n",
            "Start of epoch 293\n",
            "Training loss: 1.7979\n",
            "\n",
            "Start of epoch 294\n",
            "Training loss: 1.7892\n",
            "\n",
            "Start of epoch 295\n",
            "Training loss: 1.7805\n",
            "\n",
            "Start of epoch 296\n",
            "Training loss: 1.7719\n",
            "\n",
            "Start of epoch 297\n",
            "Training loss: 1.7632\n",
            "\n",
            "Start of epoch 298\n",
            "Training loss: 1.7546\n",
            "\n",
            "Start of epoch 299\n",
            "Training loss: 1.7461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRkts0vigb1B"
      },
      "source": [
        "## **6. Some intuition 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_tJLKAagzwB"
      },
      "source": [
        "\\**Question:** Em đang chưa hiểu lắm về việc tại sao người ta thường dùng `in_embedding` để làm transferring embedding cho model khác ạ?\n",
        "\n",
        "\n",
        "*Possible intutition:* Với cặp từ 'đầu', 'lòng' thì tasks của 2 Embedding layers là làm cho vector tương ứng với 'đầu' ở in_embed giống với vector của 'lòng' ở out_embed. Bởi vì như thế sẽ maximize được $(u \\cdot v)$ và minimize $-log(u \\cdot v)$\n",
        "\n",
        "Bên cạnh đó, tập dataset cũng có tính đối xứng giữa inputs và labels.\n",
        "\n",
        "Vậy có phải việc lấy `in_embed` làm `transferring embedding` cho model khác là random không ạ? Nếu với tập train không nhiều samples thì có nên dùng `out_embed` để làm `transferring embedding` không ạ? Tại vì với `negative_sampling`, thì `out_embed` mỗi một lần `backpropagation` update nhiều weights hơn so với `in_embed`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkh27URvgcTP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}