{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNet_update.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **R-NET Model Implementation**\n",
        "\n",
        "In this notebook, we will walk through the implementation of an R-NET model as introduced in the paper [R-NET: Machine Reading Comprehension with Self-Matching Networks](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/r-net.pdf). R-NET is an **end-to-end neural networks model** for r**eading comprehension style question answering**. The goal of R-NET is to answer questions from a given passage. \n",
        "\n",
        "An R-NET model consists of **four parts**: \n",
        "\n",
        "1. A recurrent network **encoder** that builds representation for questions and passages separately. \n",
        "2. A **gated attention-based layer** that matches the question and passage. \n",
        "3. A **self-matching attention layer** to aggregate information from the whole passage by matching the passage against itself. \n",
        "4. A **pointer network layer** that locates the positions of answers from the passages. "
      ],
      "metadata": {
        "id": "W4eebTAzcWhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![rnet.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "h-XRxMSuccjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our notebook will construct an R-NET model by defining four separate classes corresponding to four layers and later combining them together in an R-NET class. But first, we will define a layer that build the character-level representation for a word. Our implementation is as follows:"
      ],
      "metadata": {
        "id": "SminAMZhcaMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Up**"
      ],
      "metadata": {
        "id": "TAmtN8b6crY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoiVSOuTf7T1"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import pickle\n",
        "import ujson\n",
        "from spacy.lang.en import English\n",
        "tokenizer = English().tokenizer\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import traceback\n",
        "\n",
        "from config import base_config\n",
        "use_gpu = base_config.get(\"use_gpu\",False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and use_gpu == True else \"cpu\")        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embedding layer**\n",
        "The first layer will perform the character embedding of each word. The motivation for such embedding is that character-level embeddings have been show to deal with out-of-vocab token (OOV). Together, both the character-level embedding and word-level embedding will be fed into the gated attention-based layer."
      ],
      "metadata": {
        "id": "3dNe0APmcyPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_size=300, hidden_size=75, num_layers=3, bidirectional=True,dropout=0.2):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # Define a bidirectional RNN to encode the input matrix\n",
        "        self.gru = nn.GRU(input_size=input_size,hidden_size=hidden_size, num_layers=num_layers, bidirectional=bidirectional)\n",
        "\n",
        "        # Define a dropout layer\n",
        "        if dropout is None:\n",
        "            self.dropout = None\n",
        "        else:\n",
        "            self.dropout = nn.Dropout(p=dropout)\n",
        "    \n",
        "    def forward(self, input):\n",
        "\n",
        "        # Feed the input into the RNN model\n",
        "        o, _ = self.gru(input) \n",
        "        \n",
        "        # the shape of the output is (seq_len, batch, num_directions * hidden_size)\n",
        "        # Then apply the dropout layer to the output\n",
        "        o = self.dropout(o)\n",
        "        return o"
      ],
      "metadata": {
        "id": "SeUMyrCuZ1lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gated Attention Layer**\n",
        "\n",
        "The motivation for this layer is that based on the question, there must be some parts in the passage that is extremely important. Thus, this layer tries to match the similarity between the passage and the question by determining the importance of passage parts and attend to ones relevant to the question.\n",
        "\n",
        "The model does this by calculating the score between each token in the question and the passage to determine which parts are important between them (similar to the attention mechanism). Then, the model uses the **match-LSTM** to update the representation of each token in the passage to determine their relationship with the question."
      ],
      "metadata": {
        "id": "taSTUzWBdcX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuestionPassageCoAttention(nn.Module):\n",
        "    # Initiate the layer \n",
        "    def __init__(self, hidden_size=75, batch_size=32):  \n",
        "        super(QuestionPassageCoAttention, self).__init__()\n",
        "\n",
        "        # Define the batch size and hidden embedding size\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define the matrix correspond to the embedding of a token of the question\n",
        "        self.WQ_sub_u = torch.nn.Linear(2 * hidden_size, hidden_size)\n",
        "\n",
        "        # Define the matrix correspond to the embedding of a token of the passage\n",
        "        self.WP_sub_u = torch.nn.Linear(2 * hidden_size, hidden_size)\n",
        "\n",
        "        # Define the matrix correspond to the previous sentence-pair representation\n",
        "        self.WP_sub_v = torch.nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # Define the matrix deals with matching the important parts of passage to the question\n",
        "        self.Wg = torch.nn.Linear(4 * hidden_size, 4* hidden_size)\n",
        "\n",
        "        # Initiate the passage representation which incorporates aggregated matching information from the question.\n",
        "        self.Vt = torch.randn((self.batch_size, hidden_size, 1)).to(device)\n",
        "\n",
        "        # Define two RNN layer, one is forward, one is backward\n",
        "        self.gatedRNN_fwd = nn.GRUCell(input_size= 4*hidden_size, hidden_size=hidden_size)\n",
        "        self.gatedRNN_rev = nn.GRUCell(input_size= 4*hidden_size, hidden_size=hidden_size)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        \n",
        "    \n",
        "    def forward(self, quesEnc, passEnc):\n",
        "\n",
        "        # Collect the size of the sequence of passage encoding and question encoding\n",
        "        \"\"\"\n",
        "        lQ: length of question\n",
        "        lP: length of question\n",
        "        hiddenP: hidden size (output features size or size of last dim) of passage encoding (available here as passEnc) from the 'Encoding' Layer\n",
        "        hiddenQ: hidden size (output features size or size of last dim) of question encoding (available here as quesEnc) from the 'Encoding' Layer i.e.\n",
        "        bP: batch size received for the passEnc\n",
        "        bQ: batch size received for the quesEnc \n",
        "        \"\"\"\n",
        "        lP, bP, hiddenP = passEnc.shape\n",
        "        lQ, bQ, hiddenQ = quesEnc.shape\n",
        "        \n",
        "        # Initiate the sentence-pair representation\n",
        "        prev_vP_fwd = torch.randn((bP, self.hidden_size)).to(device) # (b,h)\n",
        "        prev_vP_rev = torch.randn((bP, self.hidden_size)).to(device) # (b,h)\n",
        "\n",
        "        # Initiate a matrix to store all there representation\n",
        "        vP_all_timesteps = torch.zeros(lP, bP, 2*self.hidden_size).to(device) # (lP,b,2h)\n",
        "        \n",
        "        \n",
        "        # save quesEnc as shape with batch first i.e. (lQ, b, 2h) -> (b, lQ, 2h) for bmm operations\n",
        "        quesEnc_bf = quesEnc.permute(1,0,2)\n",
        "\n",
        "        # Over each time-step of the passage encoding\n",
        "        for t,t_rev in zip(range(lP), reversed(range(lP))):\n",
        "\n",
        "            # Calculate the attention matrices of words in passage and question in two directions.\n",
        "            temp_fwd = torch.tanh( self.WQ_sub_u(quesEnc) + self.WP_sub_u(passEnc[t]) + self.WP_sub_v(prev_vP_fwd))\n",
        "            temp_rev = torch.tanh( self.WQ_sub_u(quesEnc) + self.WP_sub_u(passEnc[t_rev]) + self.WP_sub_v(prev_vP_rev))\n",
        "            # - Next calc uses bmm (batch multiply) operation on temp which requires batch dim first, so transform temp's dimensions\n",
        "\n",
        "            # We have to transform each of the matrices so that the batch comes first.\n",
        "            temp_fwd = temp_fwd.permute([1,0,2])\n",
        "            temp_rev = temp_rev.permute([1,0,2])\n",
        "            # - Now temp's shape should be (bQ, lQ, h)\n",
        "            # - For each entity in the batch for temp (i.e. ignore the batch dim for a second here), \n",
        "            #   to convert (lQ,h) part to a size (lQ, 1) (i.e. scalar scores over all question words), \n",
        "            #   by rules of matrix multilication, we need a matrix of size (h,1) which is self.Vt\n",
        "            # - Batch multiply temp and Vt\n",
        "            #   The result, s_t, would have a shape of (b, lQ, 1) where (lQ,1) represents the scalar score \n",
        "            #   for each question word\n",
        "\n",
        "            # Then, based on the attention matrices, we multiply it with the matrix Vt to get the scores of attention in the form of matrices.\n",
        "            s_t_fwd = torch.bmm(temp_fwd, self.Vt)\n",
        "            s_t_rev = torch.bmm(temp_rev, self.Vt)\n",
        "            # Apply the softmax function to these matrices to obtain attention-scores. This is corresponding to the at_i in the paper\n",
        "            a_t_fwd = F.softmax(s_t_fwd, dim=1)\n",
        "            a_t_rev = F.softmax(s_t_rev, dim=1)\n",
        "            # - We need to do bmm (a_t, quesEnc) to calculate the context vector which is fed into the Bidirectional RNN for this layer\n",
        "            # Reshape the attention-score matrix\n",
        "            a_t_fwd = a_t_fwd.squeeze(dim=2).unsqueeze(dim=1)\n",
        "            a_t_rev = a_t_rev.squeeze(dim=2).unsqueeze(dim=1)\n",
        "            \n",
        "            # Then, based on the attention-score, we calculate the attention-pooling matrix of the question in two directions\n",
        "            c_t_fwd = torch.bmm(a_t_fwd,quesEnc_bf) # c_t is of shape (b,l,2h)\n",
        "            c_t_rev = torch.bmm(a_t_rev,quesEnc_bf) # c_t is of shape (b,l,2h)\n",
        "            # Concatenate the attention-pooling matrix\n",
        "            c_t_concat_fwd = torch.cat((passEnc[t].unsqueeze(1), c_t_fwd), dim=2) # c_t_concat is of shape (b,1,4h)\n",
        "            c_t_concat_rev = torch.cat((passEnc[t_rev].unsqueeze(1), c_t_rev), dim=2) # c_t_concat is of shape (b,1,4h)\n",
        "\n",
        "            # Based on the concatenation, we determine the important parts of passage to the questions, we does not\n",
        "            # simply feed the concatenation to the RNN model, but we want to focus on some specific important parts\n",
        "            # between the question and the passage and store such information in the matrix Wg\n",
        "            g_t_fwd = torch.sigmoid(self.Wg(c_t_concat_fwd)) # g_t is of shape (b,1,4h)\n",
        "            g_t_rev = torch.sigmoid(self.Wg(c_t_concat_rev)) # g_t is of shape (b,1,4h)\n",
        "            \n",
        "            # Then, we apply such information to the concatenation\n",
        "            c_t_concat_star_fwd = torch.mul(g_t_fwd, c_t_concat_fwd).squeeze(1) # c_t_concat_star is of shape (b,4h)\n",
        "            c_t_concat_star_rev = torch.mul(g_t_rev, c_t_concat_rev).squeeze(1) # c_t_concat_star is of shape (b,4h)\n",
        "\n",
        "            # Based on the previous sentence-pair representation and the information between the passage and the question,\n",
        "            # we generate a new sentence-pair representation\n",
        "            prev_vP_fwd = self.gatedRNN_fwd(c_t_concat_star_fwd, prev_vP_fwd) # prev_vP_fwd shape(b,h)\n",
        "            prev_vP_rev = self.gatedRNN_rev(c_t_concat_star_rev, prev_vP_rev) # prev_vP_fwd shape(b,h)\n",
        "\n",
        "            # Store the new representation based on the question\n",
        "            vP_all_timesteps[t] = torch.cat((prev_vP_fwd, prev_vP_rev), dim=1)\n",
        "            \n",
        "            # Release used memory\n",
        "            del temp_fwd, temp_rev, s_t_fwd,s_t_rev, a_t_fwd,a_t_rev, c_t_concat_fwd,c_t_concat_rev, g_t_fwd,g_t_rev, c_t_concat_star_fwd, c_t_concat_star_rev\n",
        "        vP_all_timesteps = self.dropout(vP_all_timesteps)\n",
        "        del quesEnc_bf, prev_vP_fwd, prev_vP_rev\n",
        "        return vP_all_timesteps"
      ],
      "metadata": {
        "id": "_C5lgN3UZ7Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Self-matching Attention**\n",
        "The problem with the previous representation is that it only knows the connection between the passage and the question.\n",
        "This layer tells little information about the context because the previous representation only focus on a certain part in the passage and its surrounding. In addition, passage context is also important to the problem since there are\n",
        "lexical or syntactic divergence between the passage and the question. Therefore, we create another representation between the passage and itself based on the previous representation between the passage and question.\n",
        "\n",
        "Therefore, similar to the previous layer, this self-matching Attention layer generates the representation of each token in the passage so that it contains the information about both the relationship between the passage and the question, as well as the context of the passage itself. The principle is also similar as it calculates the score between each token in the passage and maximize the score of those that are closedly related in the passage."
      ],
      "metadata": {
        "id": "M3nbwnSqf9rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfMatchingAttention(nn.Module):\n",
        "    def __init__(self, hidden_size=75, batch_size=32):\n",
        "        super(SelfMatchingAttention, self).__init__()\n",
        "\n",
        "        # Define the batch size and the hidden size\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define some matrices\n",
        "        self.WP_sub_v = torch.nn.Linear(2*hidden_size, hidden_size)\n",
        "        self.WPhat_sub_v = torch.nn.Linear(2*hidden_size, hidden_size)\n",
        "        self.Wg = torch.nn.Linear(4 * hidden_size, 4* hidden_size)\n",
        "\n",
        "        # Initiate the matrix V^T\n",
        "        self.Vt = torch.randn((self.batch_size, hidden_size, 1)).to(device)\n",
        "\n",
        "        # Define two RNN for two ways\n",
        "        self.gatedRNN_fwd = nn.GRUCell(input_size= 4*hidden_size, hidden_size=hidden_size)\n",
        "        self.gatedRNN_rev = nn.GRUCell(input_size= 4*hidden_size, hidden_size=hidden_size)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "        \n",
        "    \n",
        "    def forward(self, passEnc):\n",
        "\n",
        "        # Collect the size of the passage encoding\n",
        "        \"\"\"\n",
        "        lQ: length of question\n",
        "        lP: length of question\n",
        "        hiddenP: hidden size (output features size or size of last dim) of passage encoding (available here as passEnc) from the 'Encoding' Layer\n",
        "        hiddenQ: hidden size (output features size or size of last dim) of question encoding (available here as quesEnc) from the 'Encoding' Layer i.e.\n",
        "        bP: batch size received for the passEnc\n",
        "        bQ: batch size received for the quesEnc\n",
        "        \"\"\"\n",
        "        lP, bP, _ = passEnc.shape\n",
        "        \n",
        "        # Initiate the self-matching representation\n",
        "        prev_hP_fwd = torch.randn((bP, self.hidden_size)).to(device) # (b,h)\n",
        "        prev_hP_rev = torch.randn((bP, self.hidden_size)).to(device) # (b,h)\n",
        "\n",
        "        # Initiate the matrix to store all the representation\n",
        "        hP_all_timesteps = torch.zeros(lP, bP, 2*self.hidden_size).to(device) # (lP,b,2h)\n",
        "        \n",
        "        # Change the dimension of the encoding in original direction and reversed direction.\n",
        "        passEnc_bf = passEnc.permute(1,0,2)\n",
        "        passEnc_rev = torch.flip(passEnc, dims=[0]) # order of passage seq reversed\n",
        "        passEnc_rev_bf = passEnc_rev.permute(1,0,2)\n",
        "\n",
        "        # Over each time-step of the passage encoding\n",
        "        for t,t_rev in zip(range(lP), reversed(range(lP))):\n",
        "            \n",
        "            # Calculate the self-matching scores\n",
        "            temp_fwd = torch.tanh( self.WP_sub_v(passEnc) + self.WPhat_sub_v(passEnc[t]))\n",
        "            temp_rev = torch.tanh( self.WP_sub_v(passEnc_rev) + self.WPhat_sub_v(passEnc_rev[t]))\n",
        "            temp_fwd = temp_fwd.permute([1,0,2])\n",
        "            temp_rev = temp_rev.permute([1,0,2])\n",
        "            s_t_fwd = torch.bmm(temp_fwd, self.Vt)\n",
        "            s_t_rev = torch.bmm(temp_rev, self.Vt)\n",
        "            \n",
        "            # Apply the softmax function to the scores\n",
        "            a_t_fwd = F.softmax(s_t_fwd, dim=1)\n",
        "            a_t_rev = F.softmax(s_t_rev, dim=1)\n",
        "            \n",
        "            # Change the dimension of the self-attention score\n",
        "            a_t_fwd = a_t_fwd.squeeze(dim=2).unsqueeze(dim=1)\n",
        "            a_t_rev = a_t_rev.squeeze(dim=2).unsqueeze(dim=1)\n",
        "            \n",
        "            # Calculate the self-context matrix \n",
        "            c_t_fwd = torch.bmm(a_t_fwd,passEnc_bf) # c_t is of shape (b,l,2h)\n",
        "            c_t_rev = torch.bmm(a_t_rev,passEnc_rev_bf) # c_t is of shape (b,l,2h)\n",
        "            \n",
        "            # Concatenate the self-context matrix with the sentence-pair representation\n",
        "            c_t_concat_fwd = torch.cat((passEnc[t].unsqueeze(1), c_t_fwd), dim=2) # c_t_concat is of shape (b,1,4h)\n",
        "            c_t_concat_rev = torch.cat((passEnc_rev[t].unsqueeze(1), c_t_rev), dim=2) # c_t_concat is of shape (b,1,4h)\n",
        "\n",
        "            # Similar to the sentence-pair representation, we only want to focus on some specific parts of the passage that\n",
        "            # that relates to some other parts of the passage and the question. Thus, we create another matrix to store such information\n",
        "            g_t_fwd = torch.sigmoid(self.Wg(c_t_concat_fwd)) # g_t is of shape (b,1,4h)\n",
        "            g_t_rev = torch.sigmoid(self.Wg(c_t_concat_rev)) # g_t is of shape (b,1,4h)\n",
        "            c_t_concat_star_fwd = torch.mul(g_t_fwd, c_t_concat_fwd).squeeze(1) # c_t_concat_star is of shape (b,4h)\n",
        "            c_t_concat_star_rev = torch.mul(g_t_rev, c_t_concat_rev).squeeze(1) # c_t_concat_star is of shape (b,4h)\n",
        "\n",
        "            # Feed the above matrices to the RNN\n",
        "            prev_hP_fwd = self.gatedRNN_fwd(c_t_concat_star_fwd, prev_hP_fwd) # prev_vP_fwd shape(b,h)\n",
        "            prev_hP_rev = self.gatedRNN_rev(c_t_concat_star_rev, prev_hP_rev) # prev_vP_fwd shape(b,h)\n",
        "\n",
        "            # Store the matrix\n",
        "            hP_all_timesteps[t] = torch.cat((prev_hP_fwd, prev_hP_rev), dim=1)\n",
        "            del temp_fwd, temp_rev, s_t_fwd,s_t_rev, a_t_fwd,a_t_rev, c_t_concat_fwd,c_t_concat_rev, g_t_fwd,g_t_rev, c_t_concat_star_fwd, c_t_concat_star_rev\n",
        "        hP_all_timesteps = self.dropout(hP_all_timesteps)\n",
        "        # del passEnc_bf, prev_hP_fwd, prev_hP_bwd, passEnc_rev\n",
        "        del passEnc_bf, passEnc_rev, prev_hP_fwd\n",
        "        return hP_all_timesteps"
      ],
      "metadata": {
        "id": "tyBLZcgGbxQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pointer Network**\n",
        "\n",
        "**Pointer network** is a neural architecture that learns the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. The model uses attention as a pointer to select a member of the input sequence as the output. \n",
        "\n",
        "Thus, the goal of using a pointer network layer at the end of R-NET is to predict the start and end position of the answer. **Given the newly updated passage reprentation** *hs* from the Self-Matcher Layer **and the question encoding**, the attention mechanism is utilized as a pointer to select the **start and end position of the answer** from the passage.\n",
        "\n",
        "The **Answer Pointer Network** consists of an RNN that receives an attention-pooling vector based on current predicted probability as the input. The initial hidden state of the RNN is an attention-pooling vector over the **question** representation. The RNN outputs all time-step hidden states that gives us the attention scores to calculate the predicted probability at every time step. The maximum probability indicates where the answer is at."
      ],
      "metadata": {
        "id": "dNlrDJ7fd6qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AnswerPointerNetwork(nn.Module):\n",
        "    def __init__(self,hidden_size=75,batch_size=32):\n",
        "        super(AnswerPointerNetwork, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # define parameters to calculate the attention pointer scores\n",
        "        self.WP_sub_h = nn.Linear(2*self.hidden_size,self.hidden_size)\n",
        "        self.Wa_sub_h = nn.Linear(2*self.hidden_size,self.hidden_size)\n",
        "        self.Vt1 = torch.randn((self.batch_size, self.hidden_size, 1)).to(device) # shape (b,h,1)\n",
        "\n",
        "        # define parameters to calculate rQ, the initial state of the answer RNN\n",
        "        self.Vt2 = torch.randn((self.batch_size, self.hidden_size, 1)).to(device)\n",
        "        self.WQ_sub_u = nn.Linear(2*self.hidden_size,self.hidden_size)\n",
        "        self.WQ_sub_v = nn.Linear(self.hidden_size,self.hidden_size)\n",
        "        self.VQ_sub_r = torch.randn((1,1,self.hidden_size)).to(device)\n",
        "\n",
        "        # define answer RNN as a GRU model \n",
        "        self.answerRNN = nn.GRUCell(input_size=2*hidden_size, hidden_size=2*hidden_size)\n",
        "    \n",
        "    def forward(self, passEnc, quesEnc):\n",
        "        lP,bP,_ = passEnc.shape\n",
        "        lQ,_,_ = quesEnc.shape\n",
        "        assert bP == self.batch_size\n",
        "        \n",
        "        # Calculate the attention-pooling question vector rQ which serves as initial hidden state for Answer RNN\n",
        "        temp = torch.tanh(self.WQ_sub_u(quesEnc) + self.WQ_sub_v(self.VQ_sub_r)).permute([1,0,2]) # temp shape - (b,lQ,h)\n",
        "        sQ = torch.bmm(temp, self.Vt1) # shape (b,lQ,1)\n",
        "        a = F.softmax(sQ,dim=1) # shape (b,lQ,1)\n",
        "\n",
        "        # permute the embeddings to get correct shapes\n",
        "        quesEnc_bf = quesEnc.permute([1,0,2]) # bf: batch_first; shape (b,lQ,2h)\n",
        "        passEnc_bf = passEnc.permute([1,0,2]) # shape (b,lP,2h)\n",
        "\n",
        "        # 'a' shape (b,lQ,1); quesEnc_bf shape (b,lQ,2h) not compatible for bmm; make 'a' compatible using permute\n",
        "        a = a.permute([0,2,1]).contiguous() # shape (b,1,lQ)\n",
        "        rQ = torch.bmm(a, quesEnc_bf).squeeze(1) # shape (b,1,2h) -> squeeze(1) -> (b,2h)\n",
        "        # rQ serves as initial hidden state for the answer rnn\n",
        "\n",
        "        answer_pointers = []\n",
        "\n",
        "        # calculate the start position of the answer\n",
        "        temp2 = torch.tanh( self.WP_sub_h(passEnc) + self.Wa_sub_h(rQ)).permute([1,0,2]) # shape (b,lP,h)\n",
        "        sP = torch.bmm(temp2, self.Vt2) # sP shape (b,lP,1)\n",
        "        aP =  F.softmax(sP,dim=1).squeeze(2) # aP shape (b,lP)\n",
        "        answer_pointers.append(aP) # start of the answer\n",
        "        \n",
        "        # run answerRNN model to calculate the end position of the answer\n",
        "        aP = aP.unsqueeze(1) # shape (b,1,lP)\n",
        "        ct = torch.bmm(aP, passEnc_bf).squeeze(1) # bmm input shapes ( (b,1,lP) , (b,lP,2h) ) ; output shape (b,1,2h) -> squeeze(1) -> (b,2h)\n",
        "        rQ = self.answerRNN(rQ, ct)\n",
        "\n",
        "        temp2 = torch.tanh( self.WP_sub_h(passEnc) + self.Wa_sub_h(rQ)).permute([1,0,2]) # shape (b,lP,h)\n",
        "        sP = torch.bmm(temp2, self.Vt2) # sP shape (b,lP,1)\n",
        "        aP =  F.softmax(sP,dim=1).squeeze(2) # aP shape (b,lP)\n",
        "        answer_pointers.append(aP) # end of the answer\n",
        "        aP = aP.unsqueeze(1) # shape (b,1,lP)\n",
        "        \n",
        "        del temp,sP,aP,ct,rQ,sQ,a,quesEnc_bf,passEnc_bf\n",
        "\n",
        "        return tuple(answer_pointers)"
      ],
      "metadata": {
        "id": "xjdYAup1TUdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **R-NET Model**\n",
        "\n",
        "In this final step, we combine the four layers *encoder, pqmatcher, selfmatcher* and *pointer* together to create a complete R-NET model. Particularly, the R-NET model first takes in the embeddings of the passage and the question through the **Encoder** layer, which later builds up the representation for the passage and the question separately using a single BiRNN. The new passage representation is the input for the **PR_CoAttention Layer** that incorporates matching information from the question to the passage into a single vector. This vector is passed through a **Self-matching Attention Layer** and updated with information from the whole passage. A new vector representation of the passage is then produced with information from both the question-answer alignments and the passage as a whole. Eventually, this newly updated vector reprentation of the passage is combined to get the start and end posistions of the answer through a **Pointer Network**. "
      ],
      "metadata": {
        "id": "Byy09g-PdxC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNetModel(nn.Module):\n",
        "    def __init__(self, batch_size=None, hidden_size=75,word_emb_dim=300):\n",
        "        super(RNetModel, self).__init__()\n",
        "        if batch_size is None:\n",
        "            batch_size = base_config.get('training_batch_size')\n",
        "\n",
        "        # define four layers within RNET\n",
        "        self.encoder = Encoder(input_size=word_emb_dim, hidden_size=hidden_size,num_layers=3, bidirectional=True,dropout=0.2)\n",
        "        self.QP_CoAttention = QuestionPassageCoAttention(hidden_size=hidden_size, batch_size=batch_size)\n",
        "        self.selfMatchingAttention = SelfMatchingAttention(hidden_size=hidden_size, batch_size=batch_size)\n",
        "        self.answerPointer = AnswerPointerNetwork(hidden_size=hidden_size, batch_size=batch_size)\n",
        "    \n",
        "    def forward(self, passEmbs, quesEmbs):\n",
        "        # Move input to GPU, if available\n",
        "        passEmbs,quesEmbs = passEmbs.to(device), quesEmbs.to(device)\n",
        "\n",
        "        # encoder takes in passage/question embedding \n",
        "        # and returns new representations \n",
        "        uP = self.encoder(passEmbs)\n",
        "        uQ = self.encoder(quesEmbs)\n",
        "\n",
        "        # PQ matcher takes in the outputs of encoder \n",
        "        # and returns a single vector that contains matching info between passage and question\n",
        "        vP = self.QP_CoAttention(uQ, uP)\n",
        "\n",
        "        # self matcher takes in the output of PQ matcher\n",
        "        # and returns a self-matching question-aware passage representation \n",
        "        hP = self.selfMatchingAttention(vP)\n",
        "\n",
        "        # pointer network takes in the updated passage representation hP and the question embedding\n",
        "        # returns start and end positions of answer\n",
        "        return self.answerPointer(hP, uQ)"
      ],
      "metadata": {
        "id": "Pxl2Mi7XdI6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}