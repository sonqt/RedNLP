{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0KCfy40xMDs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, dim_h, dim_s, dim_c):\n",
        "        \"\"\"\n",
        "        dim_h: the number of features of each hidden layer of the encoder\n",
        "        dim_s: the number of features of each hidden layer of the decoder\n",
        "        dim_c: the number of features of the output from the combination of \n",
        "               the previous two vectors\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim_h = dim_h\n",
        "        self.dim_s = dim_s\n",
        "        self.dim_c = dim_c\n",
        "\n",
        "        # The first layer deals with the matrix correspond to the hidden layers in the encoder\n",
        "        self.w1 = nn.Linear(dim_h, dim_c, bias=False)\n",
        "\n",
        "        # The second layer deals with the matrix correspond to the hidden layers in the decoder\n",
        "        # Note that bias=True means it allows addition.\n",
        "        self.w2 = nn.Linear(dim_s, dim_c, bias=True)\n",
        "\n",
        "        # The third layer simply calculates the vector that converts the previous sum in to a vector\n",
        "        # containing score of each pair of layers\n",
        "        self.w3 = nn.Linear(dim_c, 1, bias=False)\n",
        "\n",
        "        # The last layer just convert w3 into softmax values\n",
        "        self.a_ij = nn.Softmax()\n",
        "\n",
        "    def forward(self, hidden_encodes, hidden_decode):\n",
        "        # Combine the term w1*encoders + w2*decoder\n",
        "        comb = self.w1(hidden_encodes) + self.w2(hidden_decode)\n",
        "\n",
        "        # Get the score values\n",
        "        out = self.w3(comb)\n",
        "\n",
        "        # Calculate the softmax value and multiply it by the hidden layers in the encoder\n",
        "        context = torch.matmul(torch.transpose(self.a_ij(out), 0, 1), hidden_encodes)\n",
        "\n",
        "        return context"
      ],
      "metadata": {
        "id": "b2PuSOjH0QMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TESTING THE CORRECTNESS IN DIMENSION\n",
        "\n",
        "dim_h = 30\n",
        "dim_s = 20\n",
        "dim_c = 40\n",
        "\n",
        "attn = Attn(dim_h, dim_s, dim_c)\n",
        "\n",
        "hidden_encodes = torch.rand(15, dim_h)\n",
        "hidden_decode = torch.rand(1, dim_s)\n",
        "\n",
        "attn(hidden_encodes, hidden_decode).size()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZeseiVBFLZU",
        "outputId": "805b378a-891d-434e-b1b0-5696a7fab91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 30])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}